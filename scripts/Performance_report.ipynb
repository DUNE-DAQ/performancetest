{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad15bb0",
   "metadata": {},
   "source": [
    "# Import the modules needed, defining paths and funtions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22d8e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to run and process\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt\n",
    "from fpdf import FPDF  \n",
    "\n",
    "label_names = ['CPU Utilization (%)', \n",
    "               'Memory Bandwidth (GB/sec)',\n",
    "               'L2 Cache Misses (Million)', \n",
    "               'L3 Cache Misses (Million)',\n",
    "               'CPU Power Consumption (Watt)']\n",
    "\n",
    "pcm_columns_list = [['C0 Core C-state residency'],\n",
    "                    ['Socket0 Memory Bandwidth', 'Socket1 Memory Bandwidth'],\n",
    "                    ['Socket0 L2 Cache Misses', 'Socket1 L2 Cache Misses'], \n",
    "                    ['Socket0 L3 Cache Misses', 'Socket1 L3 Cache Misses'], \n",
    "                    ['Package Joules Consumed Socket0 Energy Consumption', 'Package Joules Consumed Socket1 Energy Consumption']]\n",
    "uprof_columns_list = [['CPU Utilization'], \n",
    "                      ['Total Mem Bw (GB/s) Socket0', 'Total Mem Bw (GB/s) Socket1'],\n",
    "                      ['L2 Miss (pti) Socket0', 'L2 Miss (pti) Socket1'],\n",
    "                      ['L3 Miss Socket0', 'L3 Miss Socket1'], \n",
    "                      ['socket0-package-power','socket1-package-power']]\n",
    "alma9_os = ['np02srv004']\n",
    "\n",
    "#for pcm ['Socket0 L2 Cache Misses Per Instruction', 'Socket1 L2 Cache Misses Per Instruction']\n",
    "#for uprof [' Utilization (%) Socket0', 'Utilization (%) Socket1', 'L2 Hit Ratio Socket0', 'L2 Hit Ratio Socket1']\n",
    "\n",
    "label_columns = ['Socket0', 'Socket1'] \n",
    "color_list = ['red', 'blue', 'green', 'cyan', 'orange', 'yellow', 'magenta', 'lime', 'purple', 'navy', 'hotpink', 'olive', 'salmon', 'teal', 'darkblue', 'darkgreen', 'darkcyan', 'darkorange', \n",
    "              'deepskyblue', 'darkmagenta', 'sienna', 'chocolate', 'orangered', 'gray', 'royalblue', 'gold', 'peru', 'seagreen', 'violet', 'tomato', 'lightsalmon', 'crimson', 'lightblue', \n",
    "              'lightgreen', 'lightpink', 'black', 'darkgray', 'lightgray', 'saddlebrown', 'brown', 'khaki', 'tan', 'turquoise', 'linen', 'lawngreen', 'coral']\n",
    "linestyle_list = ['solid', 'dotted', 'dashed', 'dashdot']\n",
    "\n",
    "marker_list = ['s','o','.','p','P','^','<','>','*','+','x','X','d','D','h','H']\n",
    "\n",
    "def directory(input_dir):\n",
    "    # Create directory (if it doesn't exist yet):\n",
    "    for dir_path in input_dir:\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "            \n",
    "def get_unix_timestamp(time_str):\n",
    "    formats = ['%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M:%S']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            timestamp = dt.strptime(time_str, fmt).timestamp()\n",
    "            return int(timestamp * 1000) if '.' in time_str else int(timestamp)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError('Invalid time format: {}'.format(time_str))\n",
    "\n",
    "def make_column_list(file, input_dir):\n",
    "    data_frame = pd.read_csv('{}/{}.csv'.format(input_dir, file))\n",
    "    columns_list = list(data_frame.columns) \n",
    "    ncoln=len(columns_list) \n",
    "    return columns_list\n",
    "\n",
    "def datenum(d, d_base):\n",
    "    t_0 = d_base.toordinal() \n",
    "    t_1 = dt.fromordinal(t_0)\n",
    "    T = (d - t_1).total_seconds()\n",
    "    return T\n",
    "\n",
    "def is_hidden(input_dir):\n",
    "    name = os.path.basename(os.path.abspath(input_dir))\n",
    "    if name.startswith('.'):\n",
    "        return 'true'\n",
    "    else:\n",
    "        return 'false'\n",
    "\n",
    "def make_file_list(input_dir):\n",
    "    file_list =  []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for name in files:\n",
    "            if is_hidden(name) == 'true':  \n",
    "                print (name, ' is hidden, trying to delete it')\n",
    "                os.remove(os.path.join(input_dir, name))\n",
    "            else:\n",
    "                file_list.append(os.path.join(input_dir, name))\n",
    "                \n",
    "    return file_list\n",
    "\n",
    "def make_name_list(input_dir):\n",
    "    l=os.listdir(input_dir)\n",
    "    name_list=[x.split('.')[0] for x in l]\n",
    "    \n",
    "    pcm_list = []\n",
    "    uprof_list = []\n",
    "    time_list = []\n",
    "    reformated_uprof_list = []\n",
    "    reformated_time_list = []\n",
    "    \n",
    "    for i, name_i in enumerate(name_list):\n",
    "        if 'reformatter_uprof-' in name_i:\n",
    "            reformated_uprof_list.append(name_i)\n",
    "        elif 'reformatter_timechart-' in name_i:\n",
    "            reformated_time_list.append(name_i)\n",
    "        elif 'uprof-' in name_i:\n",
    "            uprof_list.append(name_i)\n",
    "        elif 'timechart-' in name_i:\n",
    "            time_list.append(name_i)\n",
    "        elif 'grafana-' in name_i:\n",
    "            pcm_list.append(name_i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return pcm_list, uprof_list, time_list, reformated_uprof_list, reformated_time_list\n",
    "\n",
    "def fetch_grafana_panels(grafana_url, dashboard_uid):\n",
    "    # Get dashboard configuration\n",
    "    dashboard_url = '{}/api/dashboards/uid/{}'.format(grafana_url, dashboard_uid)   \n",
    "    response = requests.get(dashboard_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print('Error in fetch_grafana_panels: Failed to fetch dashboard data. Status code: ', response.status_code)\n",
    "        return None\n",
    "    \n",
    "    dashboard_data = response.json()\n",
    "    # Extract panels data\n",
    "    panels = dashboard_data['dashboard']['panels']\n",
    "    return panels\n",
    "\n",
    "def get_query_urls(panel, host):\n",
    "    targets = panel.get('targets', [])\n",
    "    queries = []\n",
    "    queries_label = []\n",
    "    for target in targets:\n",
    "        if 'expr' in target:\n",
    "            query = target['expr'].replace('${host}', host)\n",
    "            queries.append(query)\n",
    "            queries_label.append(target['legendFormat'])\n",
    "    \n",
    "    if queries:\n",
    "        return queries, queries_label\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_data_and_stats_from_panel(grafana_url, dashboard_uid, delta_time, host, input_dir, output_csv_file):\n",
    "    for dashboard_uid_to_use in dashboard_uid:\n",
    "        panels_data = fetch_grafana_panels(grafana_url, dashboard_uid_to_use)\n",
    "        if not panels_data:\n",
    "            print('Error in extract_data_and_stats_from_panel: Failed to fetch dashboard panels data.')\n",
    "            return\n",
    "        \n",
    "        url = '{}/api/datasources/proxy/1/api/v1/query_range'.format(grafana_url)\n",
    "        start_timestamp = get_unix_timestamp(delta_time[0])\n",
    "        end_timestamp = get_unix_timestamp(delta_time[1])\n",
    "        all_dataframes = []\n",
    "        \n",
    "        for panel_i, panel in enumerate(panels_data):\n",
    "            if 'targets' not in panel:\n",
    "                print('Skipping panel ', panel['title'], ' with no targets.')\n",
    "                continue\n",
    "            \n",
    "            query_urls, queries_label = get_query_urls(panel, host)\n",
    "            if not query_urls:\n",
    "                print('Skipping panel ', panel['title'], ' with no valid query URL.')\n",
    "                continue\n",
    "            \n",
    "            for i, query_url in enumerate(query_urls):\n",
    "                column_name = '{} {}'.format(queries_label[i], panel['title'])\n",
    "                data = {\n",
    "                    'query': query_url,\n",
    "                    'start': start_timestamp,\n",
    "                    'end': end_timestamp,\n",
    "                    'step': 2\n",
    "                }\n",
    "\n",
    "                response = requests.post(url, data=data)\n",
    "                response_data = response.json()\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print('Error: Failed to fetch dashboard data. Status code:content ', response.status_code, ':', response.content)\n",
    "                    print('Response panel:data:content for panel ', panel['title'], ':', response_data, ':', response.content)\n",
    "                    return None\n",
    "\n",
    "                if 'data' not in response_data or 'resultType' not in response_data['data'] or response_data['data']['resultType'] != 'matrix':\n",
    "                    print('Skipping query with no valid response in panel: ', panel['title'])\n",
    "                    continue\n",
    "\n",
    "                result = response_data['data']['result'][0]\n",
    "                metric = result['metric']\n",
    "                values = result.get('values', [])\n",
    "                values_without_first_column = [row[1:] for row in values]\n",
    "\n",
    "                if not values:\n",
    "                    print('Skipping query with no valid response in panel: ', panel['title'])\n",
    "                    continue\n",
    "\n",
    "                timestamps = [val[0] for val in values]\n",
    "                df_first = pd.DataFrame(values, columns=['Timestamp', column_name])\n",
    "                df_first['Timestamp'] = pd.to_datetime(df_first['Timestamp'], unit='s')\n",
    "                df = pd.DataFrame(values_without_first_column, columns=[column_name])\n",
    "                \n",
    "                if panel_i == 0 and i == 0: \n",
    "                    df_tmp = df_first\n",
    "                else:\n",
    "                    df_tmp = df\n",
    "                \n",
    "                all_dataframes.append(df_tmp)\n",
    "\n",
    "        # Combine all dataframes into a single dataframe\n",
    "        combined_df = pd.concat(all_dataframes, axis=1)\n",
    "\n",
    "        # Save the combined dataframe as a CSV file\n",
    "        output = '{}/grafana-{}.csv'.format(input_dir, output_csv_file)\n",
    "        try:\n",
    "            combined_df.to_csv(output, index=False)\n",
    "            print('Data saved to CSV successfully:', output)\n",
    "        except Exception as e:\n",
    "            print('Exception Error: Failed to save data to CSV:', str(e))\n",
    "\n",
    "def uprof_pcm_formatter(input_dir, file):\n",
    "    #data_frame = pd.read_csv('{}/{}.csv'.format(input_dir, file), skiprows=[1, 47], error_bad_lines=False) \n",
    "\n",
    "    newfile = 'reformatter_{}'.format(file)\n",
    "    f = open('{}/{}.csv'.format(input_dir, file),'r')\n",
    "    f_new = open('{}/{}.csv'.format(input_dir, newfile),'w')\n",
    "    \n",
    "    for line in f:\n",
    "        # extract initial time\n",
    "        if 'Profile Time:' in line:\n",
    "            full_date = line[14:-1]\n",
    "            full_date = full_date.replace('/', '-')\n",
    "            msec0 = int(full_date[20:23])\n",
    "            sec0  = int(full_date[17:19])\n",
    "            min0  = int(full_date[14:16])\n",
    "            hour0 = int(full_date[11:13])\n",
    "            day0  = int(full_date[8:10])\n",
    "        \n",
    "        # append package numbers to headers,\n",
    "        # and add headers for l2 cache hit ratio\n",
    "        if 'Package' in line:\n",
    "            header1 = line.split(',')\n",
    "        if 'Timestamp' in line:\n",
    "            header2 = line.split(',')[1:]\n",
    "\n",
    "            package_num = '0'\n",
    "            header_new = ['Timestamp']\n",
    "            for package,header in zip(header1,header2):\n",
    "                if (package=='\\n') or (header=='\\n'):\n",
    "                    header_new += ['L2 Hit Ratio Socket0', 'L2 Hit Ratio Socket1', 'CPU Utilization', '\\n']\n",
    "                    header_new_str = ','.join(header_new)\n",
    "                    f_new.write(header_new_str)\n",
    "                if 'Package' in package:\n",
    "                    package_num = package[-1]\n",
    "                header_new += [header+' Socket' + package_num]\n",
    "          \n",
    "        # generate full timestamps\n",
    "        if re.search('..:..:..:...,', line):\n",
    "            msec_n_old = int(line[9:12])\n",
    "            sec_n_old = int(line[6:8])\n",
    "            min_n_old = int(line[3:5])\n",
    "            hour_n_old = int(line[0:2])\n",
    "            \n",
    "            msec_n = (msec_n_old + msec0) % 1000\n",
    "            msec_carryover = (msec_n_old + msec0) // 1000\n",
    "            sec_n  = (sec_n_old + sec0 + msec_carryover) % 60\n",
    "            sec_carryover  = (sec_n_old + sec0 + msec_carryover) // 60\n",
    "            min_n  = (min_n_old + min0 + sec_carryover) % 60\n",
    "            min_carryover = (min_n_old + min0 + sec_carryover) // 60\n",
    "            hour_n = (hour_n_old + hour0 + min_carryover) % 24\n",
    "            hour_carryover = (hour_n_old + hour0 + min_carryover) // 24\n",
    "            day_n  = (day0 + hour_carryover)\n",
    "            date_n = '{year_month}-{day:02d} {hour:02d}:{min:02d}:{sec:02d}'.format(year_month=full_date[0:7], day=day_n, hour=hour_n, min=min_n, sec=sec_n)\n",
    "            line_n = re.sub('..:..:..:...', date_n, line)\n",
    "            \n",
    "            # calculate L2 Hit ratio\n",
    "            line_list = line_n.split(',')\n",
    "            l2_hit_ratio_0 = float(line_list[19]) / (float(line_list[19]) + float(line_list[15])) * 100\n",
    "            l2_hit_ratio_0 = str(round(l2_hit_ratio_0, 2))\n",
    "            l2_hit_ratio_1 = float(line_list[41]) / (float(line_list[41]) + float(line_list[37])) * 100\n",
    "            l2_hit_ratio_1 = str(round(l2_hit_ratio_1, 2))\n",
    "\n",
    "            # CPU Utilization\n",
    "            cpu_utiliz = float(line_list[1]) + float(line_list[22])\n",
    "            cpu_utiliz = str(round(cpu_utiliz, 2))\n",
    "            \n",
    "            line_list[-1] = l2_hit_ratio_0\n",
    "            line_list.append(l2_hit_ratio_1)\n",
    "            line_list.append(cpu_utiliz)\n",
    "            line_list.append('\\n')\n",
    "            line_n = ','.join(line_list)\n",
    "            f_new.write(line_n)   \n",
    "            \n",
    "    f.close()\n",
    "    f_new.close()\n",
    "    \n",
    "def uprof_timechart_formatter(input_dir, file):\n",
    "    newfile = 'reformatter_{}'.format(file)\n",
    "    f = open('{}/{}.csv'.format(input_dir, file),'r')\n",
    "    f_new = open('{}/{}.csv'.format(input_dir, newfile),'w')\n",
    "\n",
    "    header = True\n",
    "    for line in f:\n",
    "        # get & reformat full date\n",
    "        if 'Profile Start Time:' in line:\n",
    "            full_date = line.split(',')[1]\n",
    "            month = month2num(full_date[0:3])\n",
    "            date = int(full_date[4:6])\n",
    "            year = int(full_date[7:11])\n",
    "            full_date_new = '{year}-{month:02d}-{date:02d}'.format(year=year, month=month, date=date)\n",
    "\n",
    "        # Reformat timestamps\n",
    "        if not header:\n",
    "            timestamp_n = line.split(',')[1]\n",
    "            timestamp_n = timestamp_n.split(':')\n",
    "            hour_n = int(timestamp_n[0])\n",
    "            min_n = int(timestamp_n[1])\n",
    "            sec_n = int(timestamp_n[2])\n",
    "            date_n = ',{year_month_day} {hour:02d}:{min:02d}:{sec:02d},'.format(year_month_day=full_date_new, hour=hour_n, min=min_n, sec=sec_n)\n",
    "\n",
    "            line_n = re.sub(',.*:.*:.*:...,', date_n, line)\n",
    "            f_new.write(line_n)\n",
    "\n",
    "        # header=False indicates next line is data\n",
    "        if 'Timestamp' in line:\n",
    "            header = False\n",
    "            f_new.write(line)\n",
    "\n",
    "    f.close()\n",
    "    f_new.close()\n",
    "\n",
    "def month2num(month_str):\n",
    "    if month_str=='Jan':\n",
    "        return 1\n",
    "    elif month_str=='Feb':\n",
    "        return 2\n",
    "    elif month_str=='Mar':\n",
    "        return 3\n",
    "    elif month_str=='Apr':\n",
    "        return 4\n",
    "    elif month_str=='May':\n",
    "        return 5\n",
    "    elif month_str=='Jun':\n",
    "        return 6\n",
    "    elif month_str=='Jul':\n",
    "        return 7\n",
    "    elif month_str=='Aug':\n",
    "        return 8\n",
    "    elif month_str=='Sep':\n",
    "        return 9\n",
    "    elif month_str=='Oct':\n",
    "        return 10\n",
    "    elif month_str=='Nov':\n",
    "        return 11\n",
    "    elif month_str=='Dec':\n",
    "        return 12\n",
    "    else:\n",
    "        print('Warning: invalid month')\n",
    "\n",
    "def combine_time_and_uprof_files(input_dir, time_file, uprof_file):\n",
    "    input_file0 = '{}/reformatter_{}.csv'.format(input_dir, time_file)\n",
    "    input_file1 = '{}/reformatter_{}.csv'.format(input_dir, uprof_file)\n",
    "        \n",
    "    data_frame0 = pd.read_csv(input_file0) \n",
    "    data_frame1 = pd.read_csv(input_file1) \n",
    "    df_merged = data_frame0.merge(data_frame1, how='outer')\n",
    "    \n",
    "    # Save the combined dataframe as a CSV file\n",
    "    output = '{}/reformatter_{}.csv'.format(input_dir, uprof_file)\n",
    "    try:\n",
    "        df_merged.to_csv(output, index=False)\n",
    "        print('Data saved to CSV successfully:', output)\n",
    "    except Exception as e:\n",
    "        print('Error in combine_time_and_uprof_files: Failed to save data to CSV:', str(e))\n",
    "    \n",
    "def add_new_time_format(input_dir, file):\n",
    "    data_frame = pd.read_csv('{}/{}.csv'.format(input_dir, file))  \n",
    "\n",
    "    # Add new time format\n",
    "    newtime=[]\n",
    "    x_0 = data_frame['Timestamp'][0]\n",
    "    d_0 = dt.strptime(x_0,'%Y-%m-%d %H:%M:%S')\n",
    "    for index, value in enumerate(data_frame['Timestamp']):   \n",
    "        d = dt.strptime(value,'%Y-%m-%d %H:%M:%S')\n",
    "        d_new = (datenum(d, d_0)-datenum(d_0, d_0))/60.\n",
    "        newtime.append(d_new) \n",
    "\n",
    "    data_frame.insert(0, 'NewTime', newtime, True)\n",
    "    data_frame.to_csv('{}/{}.csv'.format(input_dir, file), index=False)\n",
    "    \n",
    "def get_column_val(df, columns, labels, file):\n",
    "    val = []\n",
    "    label = []\n",
    "    info = break_file_name(file)\n",
    "    \n",
    "    for j, (columns_j, label_j) in enumerate(zip(columns, labels)):\n",
    "        if columns_j in ['NewTime', 'Timestamp']:\n",
    "            pass\n",
    "        elif columns_j in ['C0 Core C-state residency', 'CPU Utilization']:\n",
    "            Y_tmp = df[columns_j].mul(1)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {}'.format(info[5], info[2]))\n",
    "        elif columns_j in ['Socket0 L3 Cache Misses Per Instruction', 'Socket1 L3 Cache Misses Per Instruction', 'L2 Miss (pti) Socket0', 'L2 Miss (pti) Socket1']:\n",
    "            Y_tmp = df[columns_j].mul(1)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['Socket0 L2 Cache Misses', 'Socket1 L2 Cache Misses', 'Socket0 L3 Cache Misses', 'Socket1 L3 Cache Misses']:\n",
    "            Y_tmp = df[columns_j].mul(1)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['L3 Miss Socket0', 'L3 Miss Socket1']:\n",
    "            Y_tmp = df[columns_j].div(1000000000)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['Socket0 Memory Bandwidth', 'Socket1 Memory Bandwidth']:\n",
    "            Y_tmp = df[columns_j].div(1000)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['L2 Hit Ratio Socket0', 'L2 Hit Ratio Socket1']:\n",
    "            Y_tmp = df[columns_j].div(1)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['Socket0 L2 Cache Misses Per Instruction', 'Socket1 L2 Cache Misses Per Instruction']:\n",
    "            Y_tmp = df[columns_j].mul(100)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        else:\n",
    "            Y = df[columns_j].values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "    \n",
    "    return val, label\n",
    "\n",
    "def plot_vars_comparison(input_dir, output_dir, pcm_file, uprof_file):\n",
    "    X_pcm_plot = []\n",
    "    X_uprof_plot = []\n",
    "    Y_pcm_plot = []\n",
    "    Y_uprof_plot = []\n",
    "    label_pcm_plot = []\n",
    "    label_uprof_plot = []\n",
    "    \n",
    "    for i, (file_pcm_i, file_uprof_i) in enumerate(zip(pcm_file, uprof_file)):    \n",
    "        refor_uprof_file = 'reformatter_{}'.format(file_uprof_i)\n",
    "    \n",
    "        info_pcm = break_file_name(file_pcm_i)\n",
    "        info_uprof = break_file_name(refor_uprof_file)\n",
    "\n",
    "        data_frame0 = pd.read_csv('{}/{}.csv'.format(input_dir, file_pcm_i))\n",
    "        data_frame1 = pd.read_csv('{}/{}.csv'.format(input_dir, refor_uprof_file))\n",
    "\n",
    "        X_pcm_plot.append(data_frame0['NewTime'].values.tolist())\n",
    "        X_uprof_plot.append(data_frame1['NewTime'].values.tolist())\n",
    "        \n",
    "        Y_pcm_tmp = []\n",
    "        Y_uprof_tmp = []\n",
    "        label_pcm_tmp = []\n",
    "        label_uprof_tmp = []\n",
    "        color_pcm_plot = []\n",
    "        color_uprof_plot = []\n",
    "        \n",
    "        for j, (columns_pcm, columns_uprof) in enumerate(zip(pcm_columns_list, uprof_columns_list)):\n",
    "            Y_pcm, label_pcm = get_column_val(data_frame0, columns_pcm, label_columns, file_pcm_i)\n",
    "            Y_pcm_tmp.append(Y_pcm)\n",
    "            label_pcm_tmp.append(label_pcm)     \n",
    "            color_pcm_plot.append(color_list[j])\n",
    "\n",
    "            Y_uprof, label_uprof = get_column_val(data_frame1, columns_uprof, label_columns, refor_uprof_file)\n",
    "            Y_uprof_tmp.append(Y_uprof)\n",
    "            label_uprof_tmp.append(label_uprof)\n",
    "            k=len(color_list)-j-1\n",
    "            color_uprof_plot.append(color_list[k])\n",
    "\n",
    "        Y_pcm_plot.append(Y_pcm_tmp)\n",
    "        Y_uprof_plot.append(Y_uprof_tmp)\n",
    "        label_pcm_plot.append(label_pcm_tmp)\n",
    "        label_uprof_plot.append(label_uprof_tmp)\n",
    "\n",
    "    # Here we make the plot:\n",
    "    matplotlib.rcParams['font.family'] = 'DejaVu Serif'\n",
    "    #fig, axs = plt.subplots(3, 2, figsize=(15,10))\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(14,16))\n",
    "    plt.rcParams['axes.grid'] = True\n",
    "    plt.style.use('default')\n",
    "    axs = axs.flatten()\n",
    "    #gs = axs[2, 0].get_gridspec()\n",
    "    #for ax in axs[4:]:\n",
    "    #    ax.remove()\n",
    "    #axs[4]= axbig = fig.add_subplot(gs[4:])\n",
    "    \n",
    "    for i in range(len(Y_pcm_plot)):\n",
    "        for j in range(len(Y_pcm_plot[i])):\n",
    "            for k in range(len(Y_pcm_plot[i][j])):              \n",
    "                axs[j].plot(X_pcm_plot[i], Y_pcm_plot[i][j][k], color=color_pcm_plot[k], label=label_pcm_plot[i][j][k], linestyle=linestyle_list[i])\n",
    "                axs[j].plot(X_uprof_plot[i], Y_uprof_plot[i][j][k], color=color_uprof_plot[k], label=label_uprof_plot[i][j][k], linestyle=linestyle_list[i])\n",
    "                axs[j].set_ylabel('{}'.format(label_names[j]))\n",
    "                axs[j].set_xlabel('Time (min)')\n",
    "                #axs[j].legend(loc='upper left')\n",
    "                axs[j].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    plt.savefig('{}/performance_comparison_{}_{}.png'.format(output_dir, info_pcm[1], info_pcm[4]))\n",
    "    plt.close()\n",
    "    \n",
    "def break_file_name(file):\n",
    "    info=file.split('-')\n",
    "    return info\n",
    "\n",
    "def check_OS(server):\n",
    "    OS = 'Centos8'\n",
    "    if server in alma9_os:\n",
    "        OS = 'Alma9'\n",
    "    return OS     \n",
    "    \n",
    "def json_info(file, input_dir, var, pdf, if_cpupins=False, if_pdf=False):\n",
    "    pdf.write(5, '====================== {} ====================== \\n'.format(file))\n",
    "    with open('{}/{}.json'.format(input_dir, file), 'r') as f:\n",
    "        data = json.load(f)\n",
    "        if if_cpupins:\n",
    "            pins = data['daq_application']['--name {}'.format(var)]\n",
    "            info = json.dumps(pins, skipkeys = True, allow_nan = True)\n",
    "            if if_pdf:\n",
    "                pdf.write(5, 'readout: {} \\n'.format(var))\n",
    "                pdf.write(5, '{} \\n'.format(info))\n",
    "        else:   \n",
    "            for var1 in ['readout', 'hsi']:\n",
    "                pins = data[var1]\n",
    "                #info = json.dumps(pins, indent = 4, skipkeys = True, allow_nan = True)\n",
    "                info = json.dumps(pins, skipkeys = True, allow_nan = True)\n",
    "                if if_pdf:\n",
    "                    pdf.write(5, '{} \\n'.format(var1))\n",
    "                    pdf.write(5, '{} \\n'.format(info))\n",
    "                    \n",
    "def create_report_performance(input_dir, output_dir, pcm_file, time_file, uprof_file, daqconfs_cpupins_folder_parent_dir, process_pcm_files=False, process_uprof_files=False, print_info=True):    \n",
    "    directory([input_dir, output_dir])\n",
    "    now = dt.now()\n",
    "    current_dnt = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Open pdf file\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font('Arial', 'B', 16)\n",
    "    pdf.cell(40,10,'Performance Report')\n",
    "    pdf.write(5,'\\n\\n')\n",
    "    \n",
    "    # Processing the data first\n",
    "    for i, (file_pcm_i, file_time_i, file_uprof_i) in enumerate(zip(pcm_file, time_file, uprof_file)):\n",
    "        if process_pcm_files:\n",
    "            add_new_time_format(input_dir, file_pcm_i)\n",
    "        if process_uprof_files:\n",
    "            uprof_timechart_formatter(input_dir, file_time_i)\n",
    "            uprof_pcm_formatter(input_dir, file_uprof_i)\n",
    "            combine_time_and_uprof_files(input_dir, file_time_i, file_uprof_i)\n",
    "            add_new_time_format(input_dir, 'reformatter_{}'.format(file_uprof_i))\n",
    "    \n",
    "    info_pcm_basic = break_file_name(pcm_file[0])\n",
    "\n",
    "    pdf.set_font('Arial', '', 8)\n",
    "    pdf.write(5, 'dunedaq verison: fddaq-{} \\n'.format(info_pcm_basic[1]))\n",
    "    pdf.write(5, 'data format: WIB{} \\n'.format(info_pcm_basic[4]))\n",
    "    pdf.write(5, 'These tests where run for 8, 16, 24, 32, 40, and 48 streams. \\n')\n",
    "        \n",
    "    for i, (file_pcm_i, file_uprof_i) in enumerate(zip(pcm_file, uprof_file)):\n",
    "        info_pcm = break_file_name(file_pcm_i)\n",
    "        info_uprof = break_file_name(file_uprof_i)\n",
    "        \n",
    "        pdf.write(5, 'The test {} made were: \\n'.format(info_pcm[5]))\n",
    "        pdf.write(5, '- Running the readout app on server {} ({}) using NUMA NODE {} and running the rest of the apps on server {}. \\n'.format(info_pcm[2], check_OS(info_pcm[2]), info_pcm[3], info_pcm[6]))\n",
    "        pdf.write(5, '- Running the readout app on server {} ({}) using NUMA NODE {} and running the rest of the apps on server {}. \\n'.format(info_uprof[2], check_OS(info_uprof[2]), info_uprof[3], info_uprof[6]))\n",
    "      \n",
    "    pdf.write(5,'\\n')\n",
    "    plot_vars_comparison(input_dir, output_dir, pcm_file, uprof_file)\n",
    "    pdf.image('{}/performance_comparison_{}_{}.png'.format(output_dir, info_pcm[1], info_pcm[4]), w=180)\n",
    "    pdf.write(5,'The figure shows the comparison of the tests ran (mentioned before) using the metrics CPU Utilization (%), Memory Bandwidth (GB/sec), L2 Cache Misses (Million), L3 Cache Misses (Million), CPU Power Consumption (Watt).')\n",
    "    pdf.write(5,'\\n\\n')\n",
    "    \n",
    "    for i, (file_pcm_i, file_uprof_i) in enumerate(zip(pcm_file, uprof_file)):\n",
    "        info_pcm = break_file_name(file_pcm_i)\n",
    "        info_uprof = break_file_name(file_uprof_i)\n",
    "        \n",
    "        json_info(file='daqconf-{}-{}-{}'.format(info_pcm[4], info_pcm[5], info_pcm[2]), input_dir='{}/daqconfs'.format(daqconfs_cpupins_folder_parent_dir), var=' ', pdf=pdf, if_cpupins=False, if_pdf=print_info)\n",
    "        json_info(file='daqconf-{}-{}-{}'.format(info_uprof[4], info_uprof[5], info_uprof[2]), input_dir='{}/daqconfs'.format(daqconfs_cpupins_folder_parent_dir), var=' ', pdf=pdf, if_cpupins=False, if_pdf=print_info)\n",
    "        json_info(file='cpupin-{}-{}-{}'.format(info_pcm[4], info_pcm[5], info_pcm[2]), input_dir='{}/cpupins'.format(daqconfs_cpupins_folder_parent_dir), var='ru{}{}{}'.format(info_pcm[2], info_pcm[4], info_pcm[3]), pdf=pdf, if_cpupins=True, if_pdf=print_info)\n",
    "        json_info(file='cpupin-{}-{}-{}'.format(info_uprof[4], info_uprof[5], info_uprof[2]), input_dir='{}/cpupins'.format(daqconfs_cpupins_folder_parent_dir), var='ru{}{}{}'.format(info_uprof[2], info_uprof[4], info_uprof[3]), pdf=pdf, if_cpupins=True, if_pdf=print_info)\n",
    "        \n",
    "    pdf.write(10,'\\n\\n\\n')\n",
    "    pdf.write(5,'The End, made on {}'.format(current_dnt))\n",
    "    pdf.output('{}/performance_report.pdf'.format(output_dir))\n",
    "\n",
    "print('Ready to run and process')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578984c4",
   "metadata": {},
   "source": [
    "# Proccesing data from Grafana and UPROF\n",
    "Note: change the paths to fit yours\n",
    "\n",
    "To extract the data from a given dashboard in grafana:\n",
    "* 'grafana_url' is:\n",
    "    * 'http://np04-srv-009.cern.ch:3000'  (legacy)\n",
    "    * 'http://np04-srv-017.cern.ch:31023' (new) not working for now use legacy\n",
    "* 'dashboard_uid' is the unic dashboard identifiyer, you can find this information on the link of the dashboard. The dashboard_uid code is in the web link after/d/.../ \n",
    "    * for intel-r-performance-counter-monitor-intel-r-pcm dashboard dashboard_uid = '91zWmJEVk' \n",
    "* delta_time is [start, end] given in the format '%Y-%m-%d %H:%M:%S'\n",
    "* host is the name of the server in study for example: \"np02-srv-003\"     \n",
    "\n",
    "file_name: [version]-[server_app_tested]-[numa node]-[data format]-[tests_name]-[server rest_of the apps]\n",
    "* example of name: v4_1_1-np02srv003-0-eth-stream_scaling-np04srv003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e5a8c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to CSV successfully: /eos/home-d/dvargas/SWAN_projects/files_servers_performance/grafana-NFD23_09_12-np02srv003-0-eth-stream_scaling-np04srv003.csv\n",
      "Data saved to CSV successfully: /eos/home-d/dvargas/SWAN_projects/files_servers_performance/grafana-NFD23_09_12-np02srv003-0-eth-stream_scaling_recording-np04srv003.csv\n",
      "done :-)\n"
     ]
    }
   ],
   "source": [
    "grafana_url = 'http://np04-srv-009.cern.ch:3000' \n",
    "dashboard_uid = ['91zWmJEVk']\n",
    "host_used = 'np02-srv-003'  \n",
    "delta_time = [['2023-09-19 14:30:10', '2023-09-19 15:41:02'],['2023-09-19 15:49:33', '2023-09-19 17:01:30']]\n",
    "output_csv_file = ['NFD23_09_12-np02srv003-0-eth-stream_scaling-np04srv003', 'NFD23_09_12-np02srv003-0-eth-stream_scaling_recording-np04srv003']\n",
    "results_path = '/eos/home-d/dvargas/SWAN_projects/files_servers_performance'\n",
    "\n",
    "for delta_time_list, output_csv_file_list in zip(delta_time, output_csv_file):\n",
    "    extract_data_and_stats_from_panel(grafana_url, dashboard_uid, delta_time=delta_time_list, host=host_used, input_dir=results_path, output_csv_file=output_csv_file_list)\n",
    "    \n",
    "print('done :-)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60327afa",
   "metadata": {},
   "source": [
    "# Performance report\n",
    "Note: change the paths to fit yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "515d9164",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE END\n"
     ]
    }
   ],
   "source": [
    "results_path = '/eos/home-d/dvargas/SWAN_projects/files_servers_performance'\n",
    "report_path = '/eos/home-d/dvargas/SWAN_projects/reports'\n",
    "performancetest_path = '/eos/home-d/dvargas/UtilsDune/files_for_v4_perf_tests'\n",
    "pcm_list, uprof_list, time_list, reformated_uprof_list, reformated_time_list = make_name_list(results_path)\n",
    "\n",
    "create_report_performance(input_dir=results_path, output_dir=report_path, pcm_file=pcm_list, time_file=time_list, uprof_file=uprof_list, daqconfs_cpupins_folder_parent_dir=performancetest_path, process_pcm_files=True, process_uprof_files=False, print_info=True)\n",
    "\n",
    "print('THE END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bcca57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
