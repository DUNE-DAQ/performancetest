{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad15bb0",
   "metadata": {},
   "source": [
    "# Import the modules needed, defining paths and funtions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d8e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt\n",
    "from fpdf import FPDF  \n",
    "\n",
    "label_names = ['CPU Utilization (%)', \n",
    "               'Memory Bandwidth (GB/sec)',\n",
    "               'L2 Cache Misses (Million)', \n",
    "               'L3 Cache Misses (Million)',\n",
    "               'CPU Power Consumption (Watt)']\n",
    "\n",
    "pcm_columns_list = [['C0 Core C-state residency'],\n",
    "                    ['Socket0 Memory Bandwidth', 'Socket1 Memory Bandwidth'],\n",
    "                    ['Socket0 L2 Cache Misses', 'Socket1 L2 Cache Misses'], \n",
    "                    ['Socket0 L3 Cache Misses', 'Socket1 L3 Cache Misses'], \n",
    "                    ['Package Joules Consumed Socket0 Energy Consumption', 'Package Joules Consumed Socket1 Energy Consumption']]\n",
    "uprof_columns_list = [['CPU Utilization'], \n",
    "                      ['Total Mem Bw (GB/s) Socket0', 'Total Mem Bw (GB/s) Socket1'],\n",
    "                      ['L2 Miss (pti) Socket0', 'L2 Miss (pti) Socket1'],\n",
    "                      ['L3 Miss Socket0', 'L3 Miss Socket1'], \n",
    "                      ['socket0-package-power','socket1-package-power']]\n",
    "alma9_os = ['np02srv004']\n",
    "\n",
    "#for pcm ['Socket0 L2 Cache Misses Per Instruction', 'Socket1 L2 Cache Misses Per Instruction']\n",
    "#for uprof [' Utilization (%) Socket0', 'Utilization (%) Socket1', 'L2 Hit Ratio Socket0', 'L2 Hit Ratio Socket1']\n",
    "\n",
    "label_columns = ['Socket0', 'Socket1'] \n",
    "color_list = ['red', 'blue', 'green', 'cyan', 'orange', 'yellow', 'magenta', 'lime', 'purple', 'navy', 'hotpink', 'olive', 'salmon', 'teal', 'darkblue', 'darkgreen', 'darkcyan', 'darkorange', \n",
    "              'deepskyblue', 'darkmagenta', 'sienna', 'chocolate', 'orangered', 'gray', 'royalblue', 'gold', 'peru', 'seagreen', 'violet', 'tomato', 'lightsalmon', 'crimson', 'lightblue', \n",
    "              'lightgreen', 'lightpink', 'black', 'darkgray', 'lightgray', 'saddlebrown', 'brown', 'khaki', 'tan', 'turquoise', 'linen', 'lawngreen', 'coral']\n",
    "linestyle_list = ['solid', 'dotted', 'dashed', 'dashdot']\n",
    "\n",
    "marker_list = ['s','o','.','p','P','^','<','>','*','+','x','X','d','D','h','H']\n",
    "\n",
    "def directory(input_dir):\n",
    "    # Create directory (if it doesn't exist yet):\n",
    "    for dir_path in input_dir:\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "            \n",
    "def get_unix_timestamp(time_str):\n",
    "    formats = ['%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M:%S']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            timestamp = dt.strptime(time_str, fmt).timestamp()\n",
    "            return int(timestamp * 1000) if '.' in time_str else int(timestamp)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError('Invalid time format: {}'.format(time_str))\n",
    "\n",
    "def make_column_list(file, input_dir):\n",
    "    data_frame = pd.read_csv('{}/{}.csv'.format(input_dir, file))\n",
    "    columns_list = list(data_frame.columns) \n",
    "    ncoln=len(columns_list) \n",
    "    return columns_list\n",
    "\n",
    "def datenum(d, d_base):\n",
    "    t_0 = d_base.toordinal() \n",
    "    t_1 = dt.fromordinal(t_0)\n",
    "    T = (d - t_1).total_seconds()\n",
    "    return T\n",
    "\n",
    "def is_hidden(input_dir):\n",
    "    name = os.path.basename(os.path.abspath(input_dir))\n",
    "    if name.startswith('.'):\n",
    "        return 'true'\n",
    "    else:\n",
    "        return 'false'\n",
    "\n",
    "def make_file_list(input_dir):\n",
    "    file_list =  []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for name in files:\n",
    "            if is_hidden(name) == 'true':  \n",
    "                print (name, ' is hidden, trying to delete it')\n",
    "                os.remove(os.path.join(input_dir, name))\n",
    "            else:\n",
    "                file_list.append(os.path.join(input_dir, name))\n",
    "                \n",
    "    return file_list\n",
    "\n",
    "def make_name_list(input_dir):\n",
    "    l=os.listdir(input_dir)\n",
    "    name_list=[x.split('.')[0] for x in l]\n",
    "    \n",
    "    all_list = []\n",
    "    all_plots_list = []\n",
    "    pcm_list = []\n",
    "    uprof_list = []\n",
    "    time_list = []\n",
    "    reformated_uprof_list = []\n",
    "    reformated_time_list = []\n",
    "    \n",
    "    for i, name_i in enumerate(name_list):\n",
    "        if 'reformatter_uprof-' in name_i:\n",
    "            reformated_uprof_list.append(name_i)\n",
    "            all_plots_list.append(name_i)\n",
    "            \n",
    "        elif 'reformatter_timechart-' in name_i:\n",
    "            reformated_time_list.append(name_i)\n",
    "            \n",
    "        elif 'uprof-' in name_i:\n",
    "            uprof_list.append(name_i)\n",
    "            all_list.append(name_i)\n",
    "            \n",
    "        elif 'timechart-' in name_i:\n",
    "            time_list.append(name_i)\n",
    "            \n",
    "        elif 'grafana-' in name_i:\n",
    "            pcm_list.append(name_i)\n",
    "            all_list.append(name_i)\n",
    "            all_plots_list.append(name_i)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return pcm_list, uprof_list, time_list, reformated_uprof_list, reformated_time_list, all_list, all_plots_list\n",
    "\n",
    "def fetch_grafana_panels(grafana_url, dashboard_uid):\n",
    "    # Get dashboard configuration\n",
    "    dashboard_url = '{}/api/dashboards/uid/{}'.format(grafana_url, dashboard_uid)   \n",
    "    response = requests.get(dashboard_url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print('Error in fetch_grafana_panels: Failed to fetch dashboard data. Status code: ', response.status_code)\n",
    "        return None\n",
    "    \n",
    "    dashboard_data = response.json()\n",
    "    # Extract panels data\n",
    "    panels = dashboard_data['dashboard']['panels']\n",
    "    return panels\n",
    "\n",
    "def get_query_urls(panel, host):\n",
    "    targets = panel.get('targets', [])\n",
    "    queries = []\n",
    "    queries_label = []\n",
    "    for target in targets:\n",
    "        if 'expr' in target:\n",
    "            query = target['expr'].replace('${host}', host)\n",
    "            queries.append(query)\n",
    "            queries_label.append(target['legendFormat'])\n",
    "    \n",
    "    if queries:\n",
    "        return queries, queries_label\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def extract_data_and_stats_from_panel(grafana_url, dashboard_uid, delta_time, host, input_dir, output_csv_file):\n",
    "    for dashboard_uid_to_use in dashboard_uid:\n",
    "        panels_data = fetch_grafana_panels(grafana_url, dashboard_uid_to_use)\n",
    "        if not panels_data:\n",
    "            print('Error in extract_data_and_stats_from_panel: Failed to fetch dashboard panels data.')\n",
    "            return\n",
    "        \n",
    "        url = '{}/api/datasources/proxy/1/api/v1/query_range'.format(grafana_url)\n",
    "        start_timestamp = get_unix_timestamp(delta_time[0])\n",
    "        end_timestamp = get_unix_timestamp(delta_time[1])\n",
    "        all_dataframes = []\n",
    "        \n",
    "        for panel_i, panel in enumerate(panels_data):\n",
    "            if 'targets' not in panel:\n",
    "                print('Skipping panel ', panel['title'], ' with no targets.')\n",
    "                continue\n",
    "            \n",
    "            query_urls, queries_label = get_query_urls(panel, host)\n",
    "            if not query_urls:\n",
    "                print('Skipping panel ', panel['title'], ' with no valid query URL.')\n",
    "                continue\n",
    "            \n",
    "            for i, query_url in enumerate(query_urls):\n",
    "                column_name = '{} {}'.format(queries_label[i], panel['title'])\n",
    "                data = {\n",
    "                    'query': query_url,\n",
    "                    'start': start_timestamp,\n",
    "                    'end': end_timestamp,\n",
    "                    'step': 2\n",
    "                }\n",
    "\n",
    "                response = requests.post(url, data=data)\n",
    "                response_data = response.json()\n",
    "                \n",
    "                if response.status_code != 200:\n",
    "                    print('Error: Failed to fetch dashboard data. Status code:content ', response.status_code, ':', response.content)\n",
    "                    print('Response panel:data:content for panel ', panel['title'], ':', response_data, ':', response.content)\n",
    "                    return None\n",
    "\n",
    "                if 'data' not in response_data or 'resultType' not in response_data['data'] or response_data['data']['resultType'] != 'matrix':\n",
    "                    print('Skipping query with no valid response in panel: ', panel['title'])\n",
    "                    continue\n",
    "\n",
    "                result = response_data['data']['result'][0]\n",
    "                metric = result['metric']\n",
    "                values = result.get('values', [])\n",
    "                values_without_first_column = [row[1:] for row in values]\n",
    "\n",
    "                if not values:\n",
    "                    print('Skipping query with no valid response in panel: ', panel['title'])\n",
    "                    continue\n",
    "\n",
    "                timestamps = [val[0] for val in values]\n",
    "                df_first = pd.DataFrame(values, columns=['Timestamp', column_name])\n",
    "                df_first['Timestamp'] = pd.to_datetime(df_first['Timestamp'], unit='s')\n",
    "                df = pd.DataFrame(values_without_first_column, columns=[column_name])\n",
    "                \n",
    "                if panel_i == 0 and i == 0: \n",
    "                    df_tmp = df_first\n",
    "                else:\n",
    "                    df_tmp = df\n",
    "                \n",
    "                all_dataframes.append(df_tmp)\n",
    "\n",
    "        # Combine all dataframes into a single dataframe\n",
    "        combined_df = pd.concat(all_dataframes, axis=1)\n",
    "\n",
    "        # Save the combined dataframe as a CSV file\n",
    "        output = '{}/grafana-{}.csv'.format(input_dir, output_csv_file)\n",
    "        try:\n",
    "            combined_df.to_csv(output, index=False)\n",
    "            print('Data saved to CSV successfully:', output)\n",
    "        except Exception as e:\n",
    "            print('Exception Error: Failed to save data to CSV:', str(e))\n",
    "\n",
    "def uprof_pcm_formatter(input_dir, file):\n",
    "    #data_frame = pd.read_csv('{}/{}.csv'.format(input_dir, file), skiprows=[1, 47], error_bad_lines=False) \n",
    "\n",
    "    newfile = 'reformatter_{}'.format(file)\n",
    "    f = open('{}/{}.csv'.format(input_dir, file),'r')\n",
    "    f_new = open('{}/{}.csv'.format(input_dir, newfile),'w')\n",
    "    \n",
    "    for line in f:\n",
    "        # extract initial time\n",
    "        if 'Profile Time:' in line:\n",
    "            full_date = line[14:-1]\n",
    "            full_date = full_date.replace('/', '-')\n",
    "            msec0 = int(full_date[20:23])\n",
    "            sec0  = int(full_date[17:19])\n",
    "            min0  = int(full_date[14:16])\n",
    "            hour0 = int(full_date[11:13])\n",
    "            day0  = int(full_date[8:10])\n",
    "        \n",
    "        # append package numbers to headers,\n",
    "        # and add headers for l2 cache hit ratio\n",
    "        if 'Package' in line:\n",
    "            header1 = line.split(',')\n",
    "        if 'Timestamp' in line:\n",
    "            header2 = line.split(',')[1:]\n",
    "\n",
    "            package_num = '0'\n",
    "            header_new = ['Timestamp']\n",
    "            for package,header in zip(header1,header2):\n",
    "                if (package=='\\n') or (header=='\\n'):\n",
    "                    header_new += ['L2 Hit Ratio Socket0', 'L2 Hit Ratio Socket1', 'CPU Utilization', '\\n']\n",
    "                    header_new_str = ','.join(header_new)\n",
    "                    f_new.write(header_new_str)\n",
    "                if 'Package' in package:\n",
    "                    package_num = package[-1]\n",
    "                header_new += [header+' Socket' + package_num]\n",
    "          \n",
    "        # generate full timestamps\n",
    "        if re.search('..:..:..:...,', line):\n",
    "            msec_n_old = int(line[9:12])\n",
    "            sec_n_old = int(line[6:8])\n",
    "            min_n_old = int(line[3:5])\n",
    "            hour_n_old = int(line[0:2])\n",
    "            \n",
    "            msec_n = (msec_n_old + msec0) % 1000\n",
    "            msec_carryover = (msec_n_old + msec0) // 1000\n",
    "            sec_n  = (sec_n_old + sec0 + msec_carryover) % 60\n",
    "            sec_carryover  = (sec_n_old + sec0 + msec_carryover) // 60\n",
    "            min_n  = (min_n_old + min0 + sec_carryover) % 60\n",
    "            min_carryover = (min_n_old + min0 + sec_carryover) // 60\n",
    "            hour_n = (hour_n_old + hour0 + min_carryover) % 24\n",
    "            hour_carryover = (hour_n_old + hour0 + min_carryover) // 24\n",
    "            day_n  = (day0 + hour_carryover)\n",
    "            date_n = '{year_month}-{day:02d} {hour:02d}:{min:02d}:{sec:02d}'.format(year_month=full_date[0:7], day=day_n, hour=hour_n, min=min_n, sec=sec_n)\n",
    "            line_n = re.sub('..:..:..:...', date_n, line)\n",
    "            \n",
    "            # calculate L2 Hit ratio\n",
    "            line_list = line_n.split(',')\n",
    "            l2_hit_ratio_0 = float(line_list[19]) / (float(line_list[19]) + float(line_list[15])) * 100\n",
    "            l2_hit_ratio_0 = str(round(l2_hit_ratio_0, 2))\n",
    "            l2_hit_ratio_1 = float(line_list[41]) / (float(line_list[41]) + float(line_list[37])) * 100\n",
    "            l2_hit_ratio_1 = str(round(l2_hit_ratio_1, 2))\n",
    "\n",
    "            # CPU Utilization\n",
    "            cpu_utiliz = float(line_list[1]) + float(line_list[22])\n",
    "            cpu_utiliz = str(round(cpu_utiliz, 2))\n",
    "            \n",
    "            line_list[-1] = l2_hit_ratio_0\n",
    "            line_list.append(l2_hit_ratio_1)\n",
    "            line_list.append(cpu_utiliz)\n",
    "            line_list.append('\\n')\n",
    "            line_n = ','.join(line_list)\n",
    "            f_new.write(line_n)   \n",
    "            \n",
    "    f.close()\n",
    "    f_new.close()\n",
    "    \n",
    "def uprof_timechart_formatter(input_dir, file):\n",
    "    newfile = 'reformatter_{}'.format(file)\n",
    "    f = open('{}/{}.csv'.format(input_dir, file),'r')\n",
    "    f_new = open('{}/{}.csv'.format(input_dir, newfile),'w')\n",
    "\n",
    "    header = True\n",
    "    for line in f:\n",
    "        # get & reformat full date\n",
    "        if 'Profile Start Time:' in line:\n",
    "            full_date = line.split(',')[1]\n",
    "            month = month2num(full_date[0:3])\n",
    "            date = int(full_date[4:6])\n",
    "            year = int(full_date[7:11])\n",
    "            full_date_new = '{year}-{month:02d}-{date:02d}'.format(year=year, month=month, date=date)\n",
    "\n",
    "        # Reformat timestamps\n",
    "        if not header:\n",
    "            timestamp_n = line.split(',')[1]\n",
    "            timestamp_n = timestamp_n.split(':')\n",
    "            hour_n = int(timestamp_n[0])\n",
    "            min_n = int(timestamp_n[1])\n",
    "            sec_n = int(timestamp_n[2])\n",
    "            date_n = ',{year_month_day} {hour:02d}:{min:02d}:{sec:02d},'.format(year_month_day=full_date_new, hour=hour_n, min=min_n, sec=sec_n)\n",
    "\n",
    "            line_n = re.sub(',.*:.*:.*:...,', date_n, line)\n",
    "            f_new.write(line_n)\n",
    "\n",
    "        # header=False indicates next line is data\n",
    "        if 'Timestamp' in line:\n",
    "            header = False\n",
    "            f_new.write(line)\n",
    "\n",
    "    f.close()\n",
    "    f_new.close()\n",
    "\n",
    "def month2num(month_str):\n",
    "    if month_str=='Jan':\n",
    "        return 1\n",
    "    elif month_str=='Feb':\n",
    "        return 2\n",
    "    elif month_str=='Mar':\n",
    "        return 3\n",
    "    elif month_str=='Apr':\n",
    "        return 4\n",
    "    elif month_str=='May':\n",
    "        return 5\n",
    "    elif month_str=='Jun':\n",
    "        return 6\n",
    "    elif month_str=='Jul':\n",
    "        return 7\n",
    "    elif month_str=='Aug':\n",
    "        return 8\n",
    "    elif month_str=='Sep':\n",
    "        return 9\n",
    "    elif month_str=='Oct':\n",
    "        return 10\n",
    "    elif month_str=='Nov':\n",
    "        return 11\n",
    "    elif month_str=='Dec':\n",
    "        return 12\n",
    "    else:\n",
    "        print('Warning: invalid month')\n",
    "\n",
    "def combine_time_and_uprof_files(input_dir, time_file, uprof_file):\n",
    "    input_file0 = '{}/reformatter_{}.csv'.format(input_dir, time_file)\n",
    "    input_file1 = '{}/reformatter_{}.csv'.format(input_dir, uprof_file)\n",
    "        \n",
    "    data_frame0 = pd.read_csv(input_file0) \n",
    "    data_frame1 = pd.read_csv(input_file1) \n",
    "    df_merged = data_frame0.merge(data_frame1, how='outer')\n",
    "    \n",
    "    # Save the combined dataframe as a CSV file\n",
    "    output = '{}/reformatter_{}.csv'.format(input_dir, uprof_file)\n",
    "    try:\n",
    "        df_merged.to_csv(output, index=False)\n",
    "        print('Data saved to CSV successfully:', output)\n",
    "    except Exception as e:\n",
    "        print('Error in combine_time_and_uprof_files: Failed to save data to CSV:', str(e))\n",
    "\n",
    "def break_file_name(file):\n",
    "    info=file.split('-')\n",
    "    return info\n",
    "\n",
    "def check_OS(server):\n",
    "    OS = 'Centos8'\n",
    "    if server in alma9_os:\n",
    "        OS = 'Alma9'\n",
    "    return OS \n",
    "\n",
    "def add_new_time_format(input_dir, file):\n",
    "    data_frame = pd.read_csv('{}/{}.csv'.format(input_dir, file))  \n",
    "\n",
    "    # Add new time format\n",
    "    newtime=[]\n",
    "    x_0 = data_frame['Timestamp'][0]\n",
    "    d_0 = dt.strptime(x_0,'%Y-%m-%d %H:%M:%S')\n",
    "    for index, value in enumerate(data_frame['Timestamp']):   \n",
    "        d = dt.strptime(value,'%Y-%m-%d %H:%M:%S')\n",
    "        d_new = (datenum(d, d_0)-datenum(d_0, d_0))/60.\n",
    "        newtime.append(d_new) \n",
    "\n",
    "    data_frame.insert(0, 'NewTime', newtime, True)\n",
    "    data_frame.to_csv('{}/{}.csv'.format(input_dir, file), index=False)\n",
    "    \n",
    "def get_column_val(df, columns, labels, file):\n",
    "    val = []\n",
    "    label = []\n",
    "    info = break_file_name(file)\n",
    "    \n",
    "    for j, (columns_j, label_j) in enumerate(zip(columns, labels)):\n",
    "        if columns_j in ['NewTime', 'Timestamp']:\n",
    "            pass\n",
    "        elif columns_j in ['C0 Core C-state residency', 'CPU Utilization']:\n",
    "            Y_tmp = df[columns_j].mul(1)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {}'.format(info[5], info[2]))\n",
    "        elif columns_j in ['Socket0 L3 Cache Misses Per Instruction', 'Socket1 L3 Cache Misses Per Instruction', 'L2 Miss (pti) Socket0', 'L2 Miss (pti) Socket1']:\n",
    "            Y_tmp = df[columns_j].mul(1)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['Socket0 L2 Cache Misses', 'Socket1 L2 Cache Misses', 'Socket0 L3 Cache Misses', 'Socket1 L3 Cache Misses']:\n",
    "            Y_tmp = df[columns_j].mul(1)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['L3 Miss Socket0', 'L3 Miss Socket1']:\n",
    "            Y_tmp = df[columns_j].div(1000000000)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['Socket0 Memory Bandwidth', 'Socket1 Memory Bandwidth']:\n",
    "            Y_tmp = df[columns_j].div(1000)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['L2 Hit Ratio Socket0', 'L2 Hit Ratio Socket1']:\n",
    "            Y_tmp = df[columns_j].div(1)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        elif columns_j in ['Socket0 L2 Cache Misses Per Instruction', 'Socket1 L2 Cache Misses Per Instruction']:\n",
    "            Y_tmp = df[columns_j].mul(100)\n",
    "            Y = Y_tmp.values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "        else:\n",
    "            Y = df[columns_j].values.tolist()\n",
    "            val.append(Y)\n",
    "            label.append('{} {} {}'.format(info[5], info[2] , label_j))\n",
    "    \n",
    "    return val, label\n",
    "    \n",
    "def plot_vars_comparison(input_dir, output_dir):\n",
    "    X_plot = []\n",
    "    Y_plot = []\n",
    "    label_plot = []\n",
    "    \n",
    "    pcm_file, uprof_file, time_file, reformated_uprof_file, reformated_time_file, all_file, all_plots_file = make_name_list(input_dir)\n",
    "    \n",
    "    for i, file_i in enumerate(all_plots_file):    \n",
    "        info = break_file_name(file_i)\n",
    "        data_frame = pd.read_csv('{}/{}.csv'.format(input_dir, file_i))\n",
    "        X_plot.append(data_frame['NewTime'].values.tolist())\n",
    "            \n",
    "        Y_tmp = []\n",
    "        label_tmp = []\n",
    "        \n",
    "        for j, (columns_pcm, columns_uprof) in enumerate(zip(pcm_columns_list, uprof_columns_list)):\n",
    "            if info[0]=='grafana':\n",
    "                Y, label = get_column_val(data_frame, columns_pcm, label_columns, file_i)            \n",
    "            #if info[0]=='reformatter_uprof':\n",
    "            else:\n",
    "                Y, label = get_column_val(data_frame, columns_uprof, label_columns, file_i)\n",
    "                \n",
    "            Y_tmp.append(Y)\n",
    "            label_tmp.append(label)     \n",
    "    \n",
    "        Y_plot.append(Y_tmp)\n",
    "        label_plot.append(label_tmp)\n",
    "        \n",
    "    # Here we make the plot:\n",
    "    matplotlib.rcParams['font.family'] = 'DejaVu Serif'\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(14, 18))\n",
    "    plt.rcParams['axes.grid'] = True\n",
    "    plt.style.use('default')\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i in range(len(Y_plot)):\n",
    "        for j in range(len(Y_plot[i])):\n",
    "            for k in range(len(Y_plot[i][j])): \n",
    "                axs[j].plot(X_plot[i], Y_plot[i][j][k], color=color_list[i], label=label_plot[i][j][k], linestyle=linestyle_list[k])\n",
    "                axs[j].set_ylabel('{}'.format(label_names[j]))\n",
    "                axs[j].set_xlabel('Time (min)')\n",
    "                axs[j].legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('{}/performance_comparison_{}_{}.png'.format(output_dir, info[1], info[4]))\n",
    "    plt.close()    \n",
    "    \n",
    "def convert(s):\n",
    "    return list(map(lambda x: x, s))\n",
    "\n",
    "def json_info(file_daqconf, file_cpupins, input_dir, var, pdf, if_pdf=False):   \n",
    "    with open('{}/cpupins/{}.json'.format(input_dir, file_cpupins), 'r') as ff:\n",
    "        data = json.load(ff)\n",
    "        pins = data['daq_application']['--name {}'.format(var)]\n",
    "        info = json.dumps(pins, skipkeys = True, allow_nan = True)\n",
    "        data_list = json.loads(info)\n",
    "        data_threads = convert(data_list['threads'])\n",
    "        max_file = int(len(data_threads)/3)\n",
    "       \n",
    "    with open('{}/daqconfs/{}.json'.format(input_dir, file_daqconf), 'r') as f:\n",
    "        data0 = json.load(f)\n",
    "        info0 = json.dumps(data0['boot']['use_connectivity_service'], skipkeys = True, allow_nan = True)\n",
    "        info1 = json.dumps(data0['readout']['thread_pinning_file'], skipkeys = True, allow_nan = True)\n",
    "        info2 = json.dumps(data0['readout']['latency_buffer_size'], skipkeys = True, allow_nan = True)\n",
    "        info3 = json.dumps(data0['readout']['enable_raw_recording'], skipkeys = True, allow_nan = True)\n",
    "        info4 = json.dumps(data0['readout']['raw_recording_output_dir'], skipkeys = True, allow_nan = True)\n",
    "        info5 = json.dumps(data0['readout']['generate_periodic_adc_pattern'], skipkeys = True, allow_nan = True)\n",
    "        info6 = json.dumps(data0['readout']['use_fake_cards'], skipkeys = True, allow_nan = True)\n",
    "        info7 = json.dumps(data0['readout']['enable_tpg'], skipkeys = True, allow_nan = True)\n",
    "        info8 = json.dumps(data0['readout']['tpg_threshold'], skipkeys = True, allow_nan = True)\n",
    "        info9 = json.dumps(data0['readout']['tpg_algorithm'], skipkeys = True, allow_nan = True)\n",
    "        info10 = json.dumps(data0['hsi']['random_trigger_rate_hz'], skipkeys = True, allow_nan = True)\n",
    "        if data0['trigger'] == 'trigger_activity_config':\n",
    "            info11 = json.dumps(data0['trigger']['trigger_activity_config']['prescale'], skipkeys = True, allow_nan = True)\n",
    "        \n",
    "        if if_pdf:\n",
    "            pdf.write(5, 'daqconf file: {} \\n'.format(file_daqconf))\n",
    "            pdf.write(5, '    * use connectivity service: \\n'.format(info0))\n",
    "            pdf.write(5, '    * cpupin file: {} \\n'.format(info1))\n",
    "            pdf.write(5, '        - {} \\n'.format(var))\n",
    "            pdf.write(5, '        - \"parent\": \"{}\" \\n'.format(data_list['parent']))\n",
    "            pdf.write(5, '        - \"threads\": \\n')\n",
    "            \n",
    "            pdf.set_font('Times', '', 8)\n",
    "            for i in range(0, max_file):\n",
    "                pdf.write(5, '                \"{}\": {}    \"{}\": {}     \"{}\": {} \\n'.format(data_threads[i], data_list['threads'][data_threads[i]], data_threads[i+max_file], data_list['threads'][data_threads[i+max_file]], data_threads[i+2*max_file], data_list['threads'][data_threads[i+2*max_file]]))\n",
    "            \n",
    "            pdf.set_font('Times', '', 10)    \n",
    "            pdf.write(5, '    * latency buffer size: {} \\n'.format(info2))\n",
    "            pdf.write(5, '    * generate periodic adc pattern: {} \\n'.format(info5))\n",
    "            pdf.write(5, '    * use fake cards: {} \\n'.format(info6))\n",
    "            pdf.write(5, '    * raw recording: {} \\n'.format(info3))\n",
    "            pdf.write(5, '    * location of the data: {} \\n'.format(info4))\n",
    "            pdf.write(5, '    * use tpg: {} \\n'.format(info7))\n",
    "            pdf.write(5, '    * tpg threshold: {} \\n'.format(info8))\n",
    "            pdf.write(5, '    * tpg algorithm: {} \\n'.format(info9))\n",
    "            pdf.write(5, '    * trigger rate: {} \\n'.format(info10))\n",
    "            \n",
    "            if data0['trigger'] == 'trigger_activity_config':\n",
    "                pdf.write(5, '    * trigger prescale: {} \\n'.format(info11))\n",
    "            else:\n",
    "                pdf.write(5, '    * trigger prescale: False \\n')\n",
    "            \n",
    "            pdf.write(5,'\\n')\n",
    "            \n",
    "def create_report_performance(input_dir, output_dir, daqconfs_cpupins_folder_parent_dir, process_pcm_files=False, process_uprof_files=False, print_info=True):    \n",
    "    directory([input_dir, output_dir])\n",
    "    now = dt.now()\n",
    "    current_dnt = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    pcm_file, uprof_file, time_file, reformated_uprof_file, reformated_time_file, all_file, all_plots_file = make_name_list(input_dir)\n",
    "\n",
    "    # Open pdf file\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.set_font('Times', 'B', 16)\n",
    "    pdf.cell(40,10,'Performance Report')\n",
    "    pdf.write(5,'\\n\\n')\n",
    "    \n",
    "    # Processing the data first\n",
    "    for i, (file_pcm_i, file_time_i, file_uprof_i) in enumerate(zip(pcm_file, time_file, uprof_file)):\n",
    "        if process_pcm_files:\n",
    "            add_new_time_format(input_dir, file_pcm_i)\n",
    "        if process_uprof_files:\n",
    "            uprof_timechart_formatter(input_dir, file_time_i)\n",
    "            uprof_pcm_formatter(input_dir, file_uprof_i)\n",
    "            combine_time_and_uprof_files(input_dir, file_time_i, file_uprof_i)\n",
    "            add_new_time_format(input_dir, 'reformatter_{}'.format(file_uprof_i))\n",
    "    \n",
    "    info_pcm_basic = break_file_name(all_file[0])\n",
    "    \n",
    "    # creating report\n",
    "    pdf.set_font('Times', '', 10)\n",
    "    pdf.write(5, 'The tests were ran using the dunedaq verison fddaq-{} and for the WIB{} data format for 8, 16, 24, 32, 40, and 48 streams. The Figure1 shows the comparison of the tests ran (Table1) using the different metrics. \\n'.format(info_pcm_basic[1], info_pcm_basic[4]))\n",
    "    pdf.write(5,'\\n')\n",
    "    plot_vars_comparison(input_dir, output_dir)\n",
    "    pdf.image('{}/performance_comparison_{}_{}.png'.format(output_dir, info_pcm_basic[1], info_pcm_basic[4]), w=180)\n",
    "    pdf.write(5, 'Figure1. Comparison of the tests ran using the metrics CPU Utilization (%), Memory Bandwidth (GB/sec), L2 Cache Misses (Million), L3 Cache Misses (Million), CPU Power Consumption (Watt).')\n",
    "    pdf.write(5,'\\n')\n",
    "    \n",
    "    #-------------------------------------------TABLE-----------------------------------------------\n",
    "    data = []\n",
    "    headers = ['Test', 'Readout app SRV', 'OS', 'NODE', 'Other app SRV']\n",
    "    data.append(headers)\n",
    "    \n",
    "    line_height = pdf.font_size * 2\n",
    "    col_width = [pdf.epw/3.8, pdf.epw/6.5, pdf.epw/10, pdf.epw/10, pdf.epw/7]\n",
    "    \n",
    "    lh_list = [] #list with proper line_height for each row\n",
    "    use_default_height = 0 #flag\n",
    "    \n",
    "    for i, file_i in enumerate(all_file):\n",
    "        info = break_file_name(file_i)\n",
    "        line = [info[5], info[2], check_OS(info[2]), info[3], info[6]]\n",
    "        data.append(line)\n",
    "    \n",
    "    for row in data:\n",
    "        for datum in row:\n",
    "            word_list = datum.split()\n",
    "            number_of_words = len(word_list) #how many words\n",
    "            if number_of_words>2: \n",
    "                use_default_height = 1\n",
    "                new_line_height = pdf.font_size * (number_of_words) #new height change according to data \n",
    "        if not use_default_height:\n",
    "            lh_list.append(line_height)\n",
    "        else:\n",
    "            lh_list.append(new_line_height)\n",
    "            use_default_height = 0\n",
    "        \n",
    "    for j, row in enumerate(data):\n",
    "        for k, datum in enumerate(row):\n",
    "            line_height = lh_list[j] #choose right height for current row\n",
    "            pdf.multi_cell(col_width[k], line_height, datum, border=1, align='L', ln=3, max_line_height=pdf.font_size)\n",
    "               \n",
    "        pdf.ln(line_height)\n",
    "    #-------------------------------------------------------------------------------------------------\n",
    "        \n",
    "    pdf.write(5, 'Table1. Summary of the tests ran. \\n')    \n",
    "    pdf.write(5,'\\n')\n",
    "    pdf.write(5, 'Configurations: \\n', 'B')\n",
    "    \n",
    "    for i, file_i in enumerate(all_file):\n",
    "        info = break_file_name(file_i)\n",
    "        json_info(file_daqconf='daqconf-{}-{}-{}'.format(info[4], info[5], info[2]), file_cpupins='cpupin-{}-{}-{}'.format(info[4], info[5], info[2]), input_dir=daqconfs_cpupins_folder_parent_dir, var='ru{}{}{}'.format(info[2], info[4], info[3]), pdf=pdf, if_pdf=print_info)\n",
    "    \n",
    "    pdf.write(5, '\\n\\n\\n')\n",
    "    pdf.write(5, 'The End, made on {}'.format(current_dnt))\n",
    "    pdf.output('{}/performance_report.pdf'.format(output_dir))\n",
    "    \n",
    "print('Ready to run and process')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578984c4",
   "metadata": {},
   "source": [
    "# Proccesing data from Grafana\n",
    "Note: change the paths to fit yours\n",
    "\n",
    "To extract the data from a given dashboard in grafana:\n",
    "* 'grafana_url' is:\n",
    "    * 'http://np04-srv-009.cern.ch:3000'  (legacy)\n",
    "    * 'http://np04-srv-017.cern.ch:31023' (new) not working for now use legacy\n",
    "* 'dashboard_uid' is the unic dashboard identifiyer, you can find this information on the link of the dashboard. The dashboard_uid code is in the web link after/d/.../ \n",
    "    * for intel-r-performance-counter-monitor-intel-r-pcm dashboard dashboard_uid = '91zWmJEVk' \n",
    "* delta_time is [start, end] given in the format '%Y-%m-%d %H:%M:%S'\n",
    "* host is the name of the server in study for example: \"np02-srv-003\"     \n",
    "\n",
    "file_name: [version]-[server_app_tested]-[numa node]-[data format]-[tests_name]-[server rest_of the apps]\n",
    "* example of name: v4_1_1-np02srv003-0-eth-stream_scaling-np04srv003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grafana_url = 'http://np04-srv-009.cern.ch:3000' \n",
    "dashboard_uid = ['91zWmJEVk']\n",
    "host_used = 'np02-srv-003'  \n",
    "delta_time = [['2023-10-01 01:42:30', '2023-10-01 02:54:35'], \n",
    "              ['2023-10-06 10:31:52', '2023-10-06 11:42:41'], \n",
    "              ['2023-10-05 15:05:45', '2023-10-05 16:17:50'], \n",
    "              ['2023-10-05 16:23:30', '2023-10-05 17:36:40']]\n",
    "output_csv_file = ['NFD23_09_28-np02srv003-0-eth-stream_scaling-np04srv003', \n",
    "                   'NFD23_09_28-np02srv003-0-eth-stream_scaling_swtpg-np04srv003', \n",
    "                   'NFD23_09_28-np02srv003-0-eth-stream_scaling_recording-np04srv003', \n",
    "                   'NFD23_09_28-np02srv003-0-eth-stream_scaling_recording_swtpg-np04srv003']\n",
    "results_path = '../performance_results'\n",
    "\n",
    "for delta_time_list, output_csv_file_list in zip(delta_time, output_csv_file):\n",
    "    extract_data_and_stats_from_panel(grafana_url, dashboard_uid, delta_time=delta_time_list, host=host_used, input_dir=results_path, output_csv_file=output_csv_file_list)\n",
    "    \n",
    "print('done :-)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60327afa",
   "metadata": {},
   "source": [
    "# Performance report\n",
    "Note: change the paths to fit yours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d9164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_path = '../performance_results'\n",
    "report_path = '../reports'\n",
    "performancetest_path = '../sourcecode/performancetest'\n",
    "\n",
    "create_report_performance(input_dir=results_path, output_dir=report_path, daqconfs_cpupins_folder_parent_dir=performancetest_path, process_pcm_files=False, process_uprof_files=False, print_info=True)\n",
    "\n",
    "print('THE END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4db16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
