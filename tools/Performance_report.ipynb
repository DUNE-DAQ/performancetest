{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237e1111",
   "metadata": {},
   "source": [
    "# Notebook for the performance test report \n",
    "This Python code is used to generate a performance report PDF from PCM/uprof monitoring data collected during readout application tests.\n",
    "\n",
    "The key functions are:\n",
    "* plot_vars_comparison(): Plots performance metrics from PCM/uprof data for multiple tests into comparison plots. It generates a plot for each socket.\n",
    "* create_report_performance(): Creates the full PDF report.\n",
    "* Processes the raw PCM/uprof data if needed\n",
    "    * Generates the comparison plots by calling plot_vars_comparison()\n",
    "    * Adds intro text, table of tests, and the plots to the PDF\n",
    "    * Prints CPU core pinning info for each test\n",
    "    * It takes input data from a specified folder, processes it, generates plots in an output folder, and builds the PDF report with custom text, table, and plots.\n",
    "    \n",
    "Helper functions all bdefined in basic_functions.py:\n",
    "\n",
    "* make_name_list(): Generates lists of file names in the input folder\n",
    "* break_file_name(): Parses info from a file name\n",
    "* add_new_time_format(): Adds a timestamp column to PCM/uprof data\n",
    "* uprof_pcm_formatter(): Converts uprof data to PCM-like format\n",
    "* json_info(): Prints CPU pinning info for a test\n",
    "\n",
    "So in summary, it automates generating a performance report from raw monitoring data, including custom intro text, test info table, comparison plots, and configuration details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebfdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modules needed, defining paths and functions\n",
    "from basic_functions_new import *\n",
    "\n",
    "print('Cheking list of packages need it')\n",
    "for package_i in list_py_package:\n",
    "    debug_missing_module(module_name=package_i)\n",
    "\n",
    "pcm_columns_list_0 = ['C0 Core C-state residency',\n",
    "                      'Socket0 Memory Bandwidth',\n",
    "                      'Socket0 Instructions Per Cycle',\n",
    "                      'Socket0 Instructions Retired Any (Million)',\n",
    "                      'Socket0 L2 Cache Misses',\n",
    "                      'Socket0 L2 Cache Hits',\n",
    "                      'Socket0 L3 Cache Misses',\n",
    "                      'Socket0 L3 Cache Hits']\n",
    "pcm_columns_list_1 = ['C0 Core C-state residency',\n",
    "                      'Socket1 Memory Bandwidth',\n",
    "                      'Socket1 Instructions Per Cycle', \n",
    "                      'Socket1 Instructions Retired Any (Million)',\n",
    "                      'Socket1 L2 Cache Misses',\n",
    "                      'Socket1 L2 Cache Hits',\n",
    "                      'Socket1 L3 Cache Misses',\n",
    "                      'Socket1 L3 Cache Hits']\n",
    "uprof_columns_list_0 = [' Utilization (%) Socket0',\n",
    "                        'Total Mem Bw (GB/s) Socket0',\n",
    "                        'IPC (Sys + User) Socket0', \n",
    "                        ' ', \n",
    "                        'L2 Miss (pti) Socket0',\n",
    "                        'L2 Access (pti) Socket0',\n",
    "                        'L3 Miss Socket0',\n",
    "                        'L3 Miss % Socket0']\n",
    "uprof_columns_list_1 = ['Utilization (%) Socket1',\n",
    "                        'Total Mem Bw (GB/s) Socket1',\n",
    "                        'IPC (Sys + User) Socket1',\n",
    "                        ' ',\n",
    "                        'L2 Miss (pti) Socket1',\n",
    "                        'L2 Access (pti) Socket1',\n",
    "                        'L3 Miss Socket1',\n",
    "                        'L3 Miss % Socket1']\n",
    "label_names = ['CPU Utilization (%)',\n",
    "               'Memory Bandwidth (GB/sec)',\n",
    "               'Instructions Per Cycle',\n",
    "               'Instructions Retired Any (Million)',\n",
    "               'L2 Cache Misses (Million)',\n",
    "               'L2 Cache [Misses/Accesses] (%)',\n",
    "               'L3 Cache Misses (Million)',\n",
    "               'L3 Cache [Misses/Accesses] (%)']\n",
    "label_columns = ['Socket0','Socket1']\n",
    "\n",
    "def plot_vars_comparison(input_dir, output_dir, all_files, pdf_name):\n",
    "    X_plot, Y_plot_0, Y_plot_1, label_plot_0, label_plot_1 = [], [], [], [], []\n",
    "    \n",
    "    for i, file_i in enumerate(all_files):    \n",
    "        info = break_file_name(file_i)\n",
    "        data_frame = pd.read_csv(f'{input_dir}/{file_i}.csv')\n",
    "        X_plot.append(data_frame['NewTime'].values.tolist())\n",
    "                \n",
    "        Y_tmp_0, Y_tmp_1, label_tmp_0, label_tmp_1 = [], [], [], []\n",
    "        \n",
    "        if info[0]=='grafana':\n",
    "            for k, (columns_pcm_0, columns_pcm_1) in enumerate(zip(pcm_columns_list_0, pcm_columns_list_1)):\n",
    "                Y_0, label_0 = get_column_val(data_frame, [columns_pcm_0], [label_columns[0]], file_i)  \n",
    "                Y_1, label_1 = get_column_val(data_frame, [columns_pcm_1], [label_columns[1]], file_i)  \n",
    "                Y_tmp_0.append(Y_0)\n",
    "                label_tmp_0.append(label_0)\n",
    "                Y_tmp_1.append(Y_1)\n",
    "                label_tmp_1.append(label_1)\n",
    "        else:\n",
    "            for k, (columns_uprof_0, columns_uprof_1) in enumerate(zip(uprof_columns_list_0, uprof_columns_list_1)):\n",
    "                Y_0, label_0 = get_column_val(data_frame, [columns_uprof_0], [label_columns[0]], file_i)\n",
    "                Y_1, label_1 = get_column_val(data_frame, [columns_uprof_1], [label_columns[1]], file_i)\n",
    "                Y_tmp_0.append(Y_0)\n",
    "                label_tmp_0.append(label_0)\n",
    "                Y_tmp_1.append(Y_1)\n",
    "                label_tmp_1.append(label_1)\n",
    "    \n",
    "        Y_plot_0.append(Y_tmp_0)\n",
    "        label_plot_0.append(label_tmp_0)\n",
    "        Y_plot_1.append(Y_tmp_1)\n",
    "        label_plot_1.append(label_tmp_1)\n",
    "    \n",
    "    # Here we make the plot:\n",
    "    matplotlib.rcParams['font.family'] = 'DejaVu Serif'\n",
    "    rows=cols=2\n",
    "    rows_cols = rows*cols\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(18, 8))\n",
    "    plt.style.use('default')\n",
    "    axs = axs.flatten()\n",
    "    #axs[3].axis('off')\n",
    "    \n",
    "    for i in range(len(Y_plot_0)):  #number of files or tests\n",
    "        for j in range(len(Y_plot_0[i])):  #number of metrix\n",
    "            if j < rows_cols:\n",
    "                label0_ij0 = re.sub('_', ' ', label_plot_0[i][j][0])\n",
    "                axs[j].plot(X_plot[i], Y_plot_0[i][j][0], color=color_list[i], label=label0_ij0, linestyle=linestyle_list[0])\n",
    "                axs[j].set_ylabel(f'{label_names[j]}')\n",
    "                axs[j].set_xlabel('Time (min)')\n",
    "                axs[j].grid(which='major', color='gray', linestyle='dashed')\n",
    "                axs[j].legend(loc='upper left')\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/Fig0_{pdf_name}_results_socket0.png')\n",
    "    print(f'{output_dir}/Fig0_{pdf_name}_results_socket0.png')\n",
    "    plt.close() \n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(18, 8))\n",
    "    plt.style.use('default')\n",
    "    axs = axs.flatten()   \n",
    "    \n",
    "    for i in range(len(Y_plot_0)):  \n",
    "        for j in range(len(Y_plot_0[i])):\n",
    "            if j < rows_cols:\n",
    "                pass\n",
    "            else:\n",
    "                label0_ij0 = re.sub('_', ' ', label_plot_0[i][j][0])\n",
    "                axs[j-rows_cols].plot(X_plot[i], Y_plot_0[i][j][0], color=color_list[i], label=label0_ij0, linestyle=linestyle_list[0])\n",
    "                axs[j-rows_cols].set_ylabel(f'{label_names[j]}')\n",
    "                axs[j-rows_cols].set_xlabel('Time (min)')\n",
    "                axs[j-rows_cols].grid(which='major', color='gray', linestyle='dashed')\n",
    "                axs[j-rows_cols].legend(loc='upper left')\n",
    "                \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/Fig1_{pdf_name}_results_cache_socket0.png')\n",
    "    print(f'{output_dir}/Fig1_{pdf_name}_results_cache_socket0.png')\n",
    "    plt.close() \n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(18, 8))\n",
    "    plt.style.use('default')\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i in range(len(Y_plot_1)):  \n",
    "        for j in range(len(Y_plot_1[i])):\n",
    "            if j < rows_cols:\n",
    "                label1_ij0 = re.sub('_', ' ', label_plot_1[i][j][0])\n",
    "                axs[j].plot(X_plot[i], Y_plot_1[i][j][0], color=color_list[i], label=label1_ij0, linestyle=linestyle_list[0])\n",
    "                axs[j].set_ylabel(f'{label_names[j]}')\n",
    "                axs[j].set_xlabel('Time (min)')\n",
    "                axs[j].grid(which='major', color='gray', linestyle='dashed')\n",
    "                axs[j].legend(loc='upper left')\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/Fig2_{pdf_name}_results_socket1.png')\n",
    "    print(f'{output_dir}/Fig2_{pdf_name}_results_socket1.png')\n",
    "    plt.close() \n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(18, 8))\n",
    "    plt.style.use('default')\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    for i in range(len(Y_plot_1)):  \n",
    "        for j in range(len(Y_plot_1[i])):\n",
    "            if j < rows_cols:\n",
    "                pass\n",
    "            else:\n",
    "                label1_ij0 = re.sub('_', ' ', label_plot_1[i][j][0])\n",
    "                axs[j-rows_cols].plot(X_plot[i], Y_plot_1[i][j][0], color=color_list[i], label=label1_ij0, linestyle=linestyle_list[0])\n",
    "                axs[j-rows_cols].set_ylabel(f'{label_names[j]}')\n",
    "                axs[j-rows_cols].set_xlabel('Time (min)')\n",
    "                axs[j-rows_cols].grid(which='major', color='gray', linestyle='dashed')\n",
    "                axs[j-rows_cols].legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/Fig3_{pdf_name}_results_cache_socket1.png')\n",
    "    print(f'{output_dir}/Fig3_{pdf_name}_results_cache_socket1.png')\n",
    "    plt.close() \n",
    "\n",
    "def create_report_performance(input_dir, output_dir, all_files, readout_name, daqconf_files, core_utilization_files, parent_folder_dir, print_info=True, pdf_name='performance_report', repin_threads_file=[None], comment=['TBA']):    \n",
    "    directory([input_dir, output_dir])\n",
    "\n",
    "    # Open pdf file\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.ln(1)\n",
    "    pdf.image(f'{parent_folder_dir}/tools/dune_logo.jpg', w=180)\n",
    "    pdf.ln(2)\n",
    "    pdf.set_font('Times', 'B', 16)\n",
    "    pdf.cell(40,10,'Performance Report')\n",
    "    pdf.ln(10)\n",
    "    \n",
    "    # creating report\n",
    "    pdf.set_font('Times', '', 10)\n",
    "    pdf.write(5, 'The tests were run for the WIBEth data format. The Figures 1 and 2 show the results of the tests ran (Table1) using the different metrics. \\n')\n",
    "    pdf.write(5, '    * L2-hits is the fraction of requests that make it to L2 at all. Similar for L3. \\n')\n",
    "    pdf.write(5, '    * L2-misses is the fraction of requests that make it to L2 at all and then miss in L2. Similar for L3. \\n')\n",
    "    pdf.ln(10)\n",
    "    \n",
    "    #-------------------------------------------TABLE-----------------------------------------------\n",
    "    # Data to tabular\n",
    "    rows_data = []\n",
    "    headers = ['Test', 'Readout SRV', 'dunedaq', 'NODE', 'General comments']\n",
    "    rows_data.append(headers)\n",
    "    \n",
    "    line_height = pdf.font_size * 2\n",
    "    col_width = [pdf.epw/3.8, pdf.epw/8, pdf.epw/7, pdf.epw/12, pdf.epw/4]  \n",
    "    lh_list = [] #list with proper line_height for each row\n",
    "    \n",
    "    for i, file_i in enumerate(all_files):\n",
    "        info = break_file_name(file_i)\n",
    "        test_info = re.sub('_', ' ', info[5])\n",
    "        line = [test_info, info[2], info[1], info[3], comment[i]]\n",
    "        rows_data.append(line)\n",
    "    \n",
    "    # Determine line heights based on the number of words in each cell\n",
    "    for row in rows_data:\n",
    "        max_lines = 1  # Initialize with a minimum of 1 line\n",
    "        for datum in row:\n",
    "            lines_needed = len(str(datum).split('\\n'))  # Count the number of lines\n",
    "            max_lines = max(max_lines, lines_needed)\n",
    "\n",
    "        lh_list.append(line_height * max_lines)\n",
    "        \n",
    "    # Add table rows with word wrapping and dynamic line heights\n",
    "    for j, row in enumerate(rows_data):\n",
    "        line_height_table = lh_list[j] \n",
    "        for k, datum in enumerate(row):\n",
    "            pdf.multi_cell(col_width[k], line_height_table, datum, border=1, align='L', new_x=XPos.RIGHT, new_y=YPos.TOP, max_line_height=pdf.font_size)\n",
    "            \n",
    "        pdf.ln(line_height_table)\n",
    "        \n",
    "    pdf.write(5, 'Table 1. Summary of the tests ran. \\n')    \n",
    "    pdf.ln(10)\n",
    "    \n",
    "    #--------------------------------------------FIGURES------------------------------------------------\n",
    "    plot_vars_comparison(input_dir, output_dir, all_files, pdf_name)\n",
    "    \n",
    "    if info[3] == '0' or info[3] == '01':\n",
    "        pdf.image(f'{output_dir}/Fig0_{pdf_name}_results_socket0.png', w=180)\n",
    "        pdf.write(5, 'Figure 1. Socket0 results of the tests ran using the metrics CPU Utilization (%), Memory Bandwidth (GB/sec), Instructions Per Cycle, Instructions Retired Any (Million).')\n",
    "        pdf.ln(10)\n",
    "        pdf.image(f'{output_dir}/Fig1_{pdf_name}_results_cache_socket0.png', w=180)\n",
    "        pdf.write(5, 'Figure 2. Socket0 results of the tests ran using the metrics L2 Cache Misses (Million), L2 Cache [Misses/Hits] (%), L3 Cache Misses (Million), and L3 Cache [Misses/Hits] (%).')\n",
    "        pdf.ln(10)\n",
    "        \n",
    "        if info[3] == '01':\n",
    "            pdf.image(f'{output_dir}/Fig2_{pdf_name}_results_socket1.png', w=180)\n",
    "            pdf.write(5, 'Figure 3. Socket1 results of the tests ran using the metrics CPU Utilization (%), Memory Bandwidth (GB/sec), Instructions Per Cycle, Instructions Retired Any (Million).')\n",
    "            pdf.ln(10)\n",
    "            pdf.image(f'{output_dir}/Fig3_{pdf_name}_results_cache_socket1.png', w=180)\n",
    "            pdf.write(5, 'Figure 4. Socket1 results of the tests ran using the metrics L2 Cache Misses (Million), L2 Cache [Misses/Hits] (%), L3 Cache Misses (Million), and L3 Cache [Misses/Hits] (%).')\n",
    "            pdf.ln(10)\n",
    "        \n",
    "    if info[3] == '1':\n",
    "        pdf.image(f'{output_dir}/Fig2_{pdf_name}_results_socket1.png', w=180)\n",
    "        pdf.write(5, 'Figure 1. Socket1 results of the tests ran using the metrics CPU Utilization (%), Memory Bandwidth (GB/sec), Instructions Per Cycle, Instructions Retired Any (Million).')\n",
    "        pdf.ln(10)\n",
    "        pdf.image(f'{output_dir}/Fig3_{pdf_name}_results_cache_socket1.png', w=180)\n",
    "        pdf.write(5, 'Figure 2. Socket1 results of the tests ran using the metrics L2 Cache Misses (Million), L2 Cache [Misses/Hits] (%), L3 Cache Misses (Million), and L3 Cache [Misses/Hits] (%).')\n",
    "        pdf.ln(10)\n",
    "        \n",
    "    #----------------------------------------CONFIGURATIONS---------------------------------------------\n",
    "    if print_info:\n",
    "        pdf.write(5, 'Configurations: \\n', 'B')\n",
    "        for i in range(len(all_files)):\n",
    "            info = break_file_name(all_files[i])\n",
    "            \n",
    "            var_i = readout_name[i]\n",
    "            file_daqconf_i = daqconf_files[i]\n",
    "            file_core_i = core_utilization_files[i]\n",
    "            repin_threads_file_i = repin_threads_file[i]\n",
    "            \n",
    "            json_info(file_daqconf=file_daqconf_i, file_core=file_core_i, parent_folder_dir=parent_folder_dir, input_dir=input_dir, var=var_i, pdf=pdf, if_pdf=print_info, repin_threads_file=repin_threads_file_i)\n",
    "            \n",
    "            #pdf.cell(0, 10, f'Table {i+2}. CPU core pins information of \"{var_i}\" for the \"{info[5]}\" test using dune_daq {info[1]}.')\n",
    "            #pdf.ln(10)           \n",
    "\n",
    "    pdf.ln(20)\n",
    "    pdf.set_font('Times', '', 10)\n",
    "    pdf.write(5, f'The End, made on {current_time()}')\n",
    "    pdf.output(f'{output_dir}/{pdf_name}_report.pdf')\n",
    "    \n",
    "    print(f'The report was create and saved to {output_dir}/{pdf_name}.pdf')\n",
    "\n",
    "print('Ready to run and process')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578984c4",
   "metadata": {},
   "source": [
    "## Proccesing data from Grafana\n",
    "To extract the data from a given dashboard in grafana (Note: change the paths to fit yours): \n",
    "* extract_grafana_data(datasource_url, grafana_url, dashboard_uid, delta_time, host, partition, input_dir, output_csv_file)\n",
    "* 'grafana_url' is:\n",
    "    * 'http://np04-srv-009.cern.ch:3000'  (legacy)\n",
    "    * 'http://np04-srv-017.cern.ch:31023' (new) \n",
    "* 'dashboard_uid' is the unique dashboard identifier, you can find this information on the link of the dashboard. The dashboard_uid code is in the web link after/d/.../. The input for the 'dashboard_uid' parameter should be a list of the dashboard IDs you want to extract data from.\n",
    "    * for intel-r-performance-counter-monitor-intel-r-pcm dashboard dashboard_uid = '91zWmJEVk' (legacy grafana)\n",
    "    * for intel-r-performance-counter-monitor-intel-r-pcm dashboard dashboard_uid = 'A_CvwTCWk' (new grafana)\n",
    "    * for daq-overview dashboard dashboard_uid = 'v4_3_0-overview'\n",
    "    * for frontend-ethernet dashboard dashboard_uid = 'v4_3_0-frontend_ethernet'\n",
    "* 'delta_time' is [start, end] given in the format '%Y-%m-%d %H:%M:%S'.\n",
    "* partition in some cases you will need to provide the partition name where the test was ran, for example 'np04-daq' or 'np04hddev'\n",
    "* host is the name of the server in study, for example: 'np02-srv-003' \n",
    "* output_csv_file (for performance tests): [version]-[server_app_tested]-[numa_node]-[data_format]-[tests_name]\n",
    "    * example of name: v4_1_1-np02srv003-0-eth-stream_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2701d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grafana_url_legacy = 'http://np04-srv-009.cern.ch:3000'\n",
    "grafana_url = 'http://np04-srv-017.cern.ch:31023'\n",
    "grafana_url_k8 = 'http://http://np04-srv-017:31003'\n",
    "prometheus_url = 'http://np04-srv-016.cern.ch:31093'\n",
    "opmon_url = 'http://opmon-influxdb.opmon.svc:8086'\n",
    "\n",
    "results_path0 = '/eos/home-d/dvargas/SWAN_projects/performance_results/2CRPs_np02srv003_and_np04srv031'\n",
    "results_path1 = '/eos/home-d/dvargas/SWAN_projects/performance_results/2CRPs_np02srv003'\n",
    "results_path2 = '/eos/home-d/dvargas/SWAN_projects/performance_results/2CRPs_np04srv031'\n",
    "\n",
    "for results_path_list in [results_path0, results_path2]:\n",
    "    extract_grafana_data(datasource_url=prometheus_url, \n",
    "                         grafana_url=grafana_url, \n",
    "                         dashboard_uid=['A_CvwTCWk'], \n",
    "                         delta_time=['2024-05-06 12:27:36', '2024-05-06 12:38:57'], \n",
    "                         host='np04-srv-031',\n",
    "                         partition='np04-daq',\n",
    "                         input_dir=results_path_list, \n",
    "                         output_csv_file='v4_4_0-np04srv031-01-eth-dual_NICs_recording')\n",
    "\n",
    "for results_path_list in [results_path0, results_path1]:\n",
    "    extract_grafana_data(datasource_url=prometheus_url, \n",
    "                         grafana_url=grafana_url, \n",
    "                         dashboard_uid=['A_CvwTCWk'], \n",
    "                         delta_time=['2024-05-06 13:20:09', '2024-05-06 13:31:14'], \n",
    "                         host='np02-srv-003',\n",
    "                         partition='np04-daq',\n",
    "                         input_dir=results_path_list, \n",
    "                         output_csv_file='v4_4_0-np02srv003-01-eth-dual_NICs_recording')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad15bb0",
   "metadata": {},
   "source": [
    "## Performance report\n",
    "To create the the performance report (Note: change the paths to fit yours): \n",
    "* process_files(input_dir, process_pcm_files=False, process_uprof_files=False, process_core_files=False). After processing the files, you want to print the lists of files you have in the folder so you can create the entries for generate the report:\n",
    "    * 'input_dir' directory where the results of the test were saved by the 'extract_grafana_data' step and where you saved the UPROF and core utilisation files too.\n",
    "    * 'process_pcm_files', 'process_uprof_files', and 'process_core_files' are to indicate which type of file should be process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d9164",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## dvargas\n",
    "\n",
    "results_path0 = '/eos/home-d/dvargas/SWAN_projects/performance_results/2CRPs_np02srv003_and_np04srv031'\n",
    "results_path1 = '/eos/home-d/dvargas/SWAN_projects/performance_results/2CRPs_np02srv003'\n",
    "results_path2 = '/eos/home-d/dvargas/SWAN_projects/performance_results/2CRPs_np04srv031'\n",
    "report_path = '/eos/home-d/dvargas/dunedaq_reports'\n",
    "performancetest_path = '/eos/home-d/dvargas/SWAN_projects/performancetest'\n",
    "\n",
    "paths = [results_path0, results_path1, results_path2]\n",
    "names = ['performancetest_2CRPs_np02srv003_and_np04srv031_comparison',\n",
    "         'performancetest_2CRPs_np02srv003', \n",
    "         'performancetest_2CRPs_np04srv031']\n",
    "comments = [['Ice lake family', \n",
    "             'Sapphire rapids family'], \n",
    "            ['Ice lake family'], \n",
    "            ['Sapphire rapids family']]\n",
    "\n",
    "for list_path in paths:\n",
    "    process_files(input_dir=list_path, process_pcm_files=False, process_uprof_files=False, process_core_files=False)\n",
    "    pcm_list, uprof_list, core_utilization_list, all_list = make_name_list(input_dir=list_path)\n",
    "    print('These are all files present in the folder ', all_list)\n",
    "\n",
    "readout_apps003 = ['runp02srv003eth0','runp02srv003eth1']\n",
    "readout_apps031 = ['runp04srv031eth0','runp04srv031eth1']\n",
    "Apps = [[readout_apps003, readout_apps031], [readout_apps003], [readout_apps031]]\n",
    "files_all = [['grafana-v4_4_0-np02srv003-01-eth-dual_NICs_recording', 'grafana-v4_4_0-np04srv031-01-eth-dual_NICs_recording'], \n",
    "             ['grafana-v4_4_0-np02srv003-01-eth-dual_NICs_recording'], ['grafana-v4_4_0-np04srv031-01-eth-dual_NICs_recording']]\n",
    "files_daqconf = [['np02daq_eth_dualNICs-recording', 'np02daq_eth_dualNICs-recording'],\n",
    "                 ['np02daq_eth_dualNICs-recording'], ['np02daq_eth_dualNICs-recording']]\n",
    "files_core_utilization = [['reformatter_core_utilization-DualNICs_np02srv003_01', \n",
    "                           'reformatter_core_utilization-DualNICs_np04srv031_01'],\n",
    "                          ['reformatter_core_utilization-DualNICs_np02srv003_01'],\n",
    "                          ['reformatter_core_utilization-DualNICs_np04srv031_01']]\n",
    "repin_threads = [[None, None], [None], [None]]\n",
    "\n",
    "print('THE END')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c1505",
   "metadata": {},
   "source": [
    "* create_report_performance(input_dir, output_dir, all_files, readout_name, daqconf_files, core_utilization_files, parent_folder_dir, print_info, pdf_name, repin_threads_file=None, comment=['general comments about the run'])\n",
    "    * 'input_dir' path to directory where the results of the test were saved by the 'extract_grafana_data' step and where you saved the UPROF and core utilisation files too.\n",
    "    * 'output_dir' path to directory where you want to store the plots and report.\n",
    "    * 'all_files' is a list of all the files in the 'input_dir' directory that you want to include in the report.\n",
    "    * 'readout_name' is a list of the names of the readout app names to be included in the report per files in 'all_files' so is a list of lists.\n",
    "    * 'daqconf_files' is a list of the configuration files for the DAQ setup used in the test per files in 'all_files' so is a list of lists.\n",
    "    * 'core_utilization_files' is a list of the core utilization files per files in 'all_files' so is a list of lists.\n",
    "    * 'parent_folder_dir' this directory refers to the folder where the 'cpupining' and 'daqconf' are present if you are running the performancetest app then it should be the performancetest folder path.\n",
    "    * 'print_info' is to set if you want the information of the configuration and cpupinning in the report. By default it will be True. \n",
    "    * 'pdf_name' is the name of the report. By default it will be 'performance_report' but you may want to give it a more descriptic name.\n",
    "    * 'repin_threads_file' in the case you are using a custom cpu pinning configuration, this is the name to the file containing the pinning information. It need to be in the same folder as the rest of cpu pinning files\n",
    "    * 'comment' is a list of strings per files in 'all_files' that will be included in the report as general comments about the run or configuration, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dvargas\n",
    "\n",
    "for list_path, list_file, list_var, list_daqconf, list_core, list_name, list_repin_threads, list_comm in zip(paths, files_all, Apps, files_daqconf, files_core_utilization, names, repin_threads, comments):\n",
    "    create_report_performance(input_dir=list_path, \n",
    "                              output_dir=report_path, \n",
    "                              all_files=list_file, \n",
    "                              readout_name=list_var, \n",
    "                              daqconf_files=list_daqconf, \n",
    "                              core_utilization_files=list_core,\n",
    "                              parent_folder_dir=performancetest_path, \n",
    "                              print_info=True, \n",
    "                              pdf_name=list_name, \n",
    "                              repin_threads_file=list_repin_threads, \n",
    "                              comment=list_comm)\n",
    "\n",
    "print('THE END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022b495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebfbd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308f4ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91b2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6e6b4da",
   "metadata": {},
   "source": [
    "#### Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress to add front end ethernet \n",
    "UIDs = [['v4_4_0-frontend_ethernet'],['v4_4_0-frontend_ethernet']]\n",
    "delta_time = [['2024-05-06 13:20:31', '2024-05-06 13:30:32'],['2024-05-06 23:42:16', '2024-05-06 23:51:23']]\n",
    "output_csv_file = ['v4_4_0-np02srv003-01-eth-test4-FE', 'v4_4_0-np02srv003-01-eth-test5-FE']\n",
    "results_path = '/eos/home-d/dvargas/SWAN_projects/performance_results/test_FE'\n",
    "\n",
    "for dashboard_uid_list, delta_time_list, output_csv_file_list in zip(UIDs, delta_time, output_csv_file):\n",
    "    extract_grafana_data(datasource_url=prometheus_url,\n",
    "                         grafana_url=grafana_url, \n",
    "                         dashboard_uid=dashboard_uid_list, \n",
    "                         delta_time=delta_time_list, \n",
    "                         host='np02-srv-003',\n",
    "                         partition='dualnic', \n",
    "                         input_dir=results_path, \n",
    "                         output_csv_file=output_csv_file_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress to add the new k8 grafana\n",
    "DS_URLs = [prometheus_url, prometheus_url]\n",
    "G_URLs = [grafana_url_k8, grafana_url_k8]\n",
    "UIDs = [['k8s_views_nodes'],['k8s_views_nodes']]\n",
    "Host = ['np02-srv-003', 'np02-srv-003']\n",
    "\n",
    "delta_time = [['2024-05-06 13:20:31', '2024-05-06 13:30:32'],['2024-05-06 23:42:16', '2024-05-06 23:51:23']]\n",
    "output_csv_file = ['v4_4_0-np02srv003-01-eth-test6', 'v4_4_0-np02srv003-01-eth-test7']\n",
    "results_path = '/eos/home-d/dvargas/SWAN_projects/performance_results/test_K8'\n",
    "\n",
    "for dashboard_uid_list, delta_time_list, output_csv_file_list in zip(UIDs, delta_time, output_csv_file):\n",
    "    extract_grafana_data(datasource_url=prometheus_url,\n",
    "                         grafana_url=grafana_url_k8, \n",
    "                         dashboard_uid=dashboard_uid_list, \n",
    "                         delta_time=delta_time_list, \n",
    "                         host='np02-srv-003', \n",
    "                         partition='prometheus-np04-daq',\n",
    "                         input_dir=results_path, \n",
    "                         output_csv_file=output_csv_file_list)\n"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
